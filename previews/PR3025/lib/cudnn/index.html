<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>cuDNN · CUDA.jl</title><meta name="title" content="cuDNN · CUDA.jl"/><meta property="og:title" content="cuDNN · CUDA.jl"/><meta property="twitter:title" content="cuDNN · CUDA.jl"/><meta name="description" content="Documentation for CUDA.jl."/><meta property="og:description" content="Documentation for CUDA.jl."/><meta property="twitter:description" content="Documentation for CUDA.jl."/><meta property="og:url" content="https://cuda.juliagpu.org/stable/lib/cudnn/"/><meta property="twitter:url" content="https://cuda.juliagpu.org/stable/lib/cudnn/"/><link rel="canonical" href="https://cuda.juliagpu.org/stable/lib/cudnn/"/><script async src="https://www.googletagmanager.com/gtag/js?id=UA-154489943-2"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-154489943-2', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="CUDA.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">CUDA.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/introduction/">Introduction</a></li><li><a class="tocitem" href="../../tutorials/custom_structs/">Using custom structs</a></li><li><a class="tocitem" href="../../tutorials/performance/">Performance Tips</a></li></ul></li><li><span class="tocitem">Installation</span><ul><li><a class="tocitem" href="../../installation/overview/">Overview</a></li><li><a class="tocitem" href="../../installation/conditional/">Conditional use</a></li><li><a class="tocitem" href="../../installation/troubleshooting/">Troubleshooting</a></li></ul></li><li><span class="tocitem">Usage</span><ul><li><a class="tocitem" href="../../usage/overview/">Overview</a></li><li><a class="tocitem" href="../../usage/workflow/">Workflow</a></li><li><a class="tocitem" href="../../usage/array/">Array programming</a></li><li><a class="tocitem" href="../../usage/memory/">Memory management</a></li><li><a class="tocitem" href="../../usage/multitasking/">Tasks and threads</a></li><li><a class="tocitem" href="../../usage/multigpu/">Multiple GPUs</a></li></ul></li><li><span class="tocitem">Development</span><ul><li><a class="tocitem" href="../../development/profiling/">Benchmarking &amp; profiling</a></li><li><a class="tocitem" href="../../development/kernel/">Kernel programming</a></li><li><a class="tocitem" href="../../development/troubleshooting/">Troubleshooting</a></li><li><a class="tocitem" href="../../development/debugging/">Debugging</a></li></ul></li><li><span class="tocitem">API reference</span><ul><li><a class="tocitem" href="../../api/essentials/">Essentials</a></li><li><a class="tocitem" href="../../api/array/">Array programming</a></li><li><a class="tocitem" href="../../api/kernel/">Kernel programming</a></li><li><a class="tocitem" href="../../api/compiler/">Compiler</a></li></ul></li><li><span class="tocitem">Library reference</span><ul><li><a class="tocitem" href="../cublas/">CUBLAS</a></li><li><a class="tocitem" href="../cudadrv/">CUDA driver</a></li><li class="is-active"><a class="tocitem" href>cuDNN</a><ul class="internal"><li><a class="tocitem" href="#Public"><span>Public</span></a></li><li><a class="tocitem" href="#Private"><span>Private</span></a></li></ul></li><li><a class="tocitem" href="../cufft/">CUFFT</a></li><li><a class="tocitem" href="../cupti/">CUPTI</a></li><li><a class="tocitem" href="../curand/">CURAND</a></li><li><a class="tocitem" href="../cusolver/">CUSOLVER</a></li><li><a class="tocitem" href="../cusparse/">CUSPARSE</a></li><li><a class="tocitem" href="../custatevec/">cuStateVec</a></li><li><a class="tocitem" href="../cutensor/">cuTENSOR</a></li><li><a class="tocitem" href="../cutensornet/">cuTensorNet</a></li><li><a class="tocitem" href="../nvml/">NVML</a></li><li><a class="tocitem" href="../utils/">APIUtils</a></li></ul></li><li><a class="tocitem" href="../../faq/">FAQ</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Library reference</a></li><li class="is-active"><a href>cuDNN</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>cuDNN</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaGPU/CUDA.jl/blob/master/docs/src/lib/cudnn.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="cuDNN"><a class="docs-heading-anchor" href="#cuDNN">cuDNN</a><a id="cuDNN-1"></a><a class="docs-heading-anchor-permalink" href="#cuDNN" title="Permalink"></a></h1><h2 id="Public"><a class="docs-heading-anchor" href="#Public">Public</a><a id="Public-1"></a><a class="docs-heading-anchor-permalink" href="#Public" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cuDNN" href="#cuDNN.cuDNN"><code>cuDNN.cuDNN</code></a> — <span class="docstring-category">Module</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cuDNN</code></pre><p>High level interface to cuDNN functions. See <a href="https://github.com/JuliaGPU/CUDA.jl/blob/master/lib/cudnn/README.md">README.md</a> for a design overview.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/cuDNN.jl#L1-L7">source</a></section></article><h2 id="Private"><a class="docs-heading-anchor" href="#Private">Private</a><a id="Private-1"></a><a class="docs-heading-anchor-permalink" href="#Private" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnDropoutSeed" href="#cuDNN.cudnnDropoutSeed"><code>cuDNN.cudnnDropoutSeed</code></a> — <span class="docstring-category">Constant</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnDropoutForward(x; dropout=0.5)
cudnnDropoutForward(x, d::cudnnDropoutDescriptor)
cudnnDropoutForward!(y, x; dropout=0.5)
cudnnDropoutForward!(y, x, d::cudnnDropoutDescriptor)</code></pre><p>Return a new array similar to <code>x</code> where approximately <code>dropout</code> fraction of the values are replaced by a 0, and the rest are scaled by <code>1/(1-dropout)</code>.  Optionally <code>y</code> holds the result and <code>d</code> specifies the operation. <code>y</code> should be similar to <code>x</code> if specified.</p><p>The user can set the global seed <code>cudnnDropoutSeed[]</code> to a positive number to always drop the same values deterministically for debugging. Note that this slows down the operation by about 40x.</p><p>The global constant <code>cudnnDropoutState::Dict</code> holds the random number generator state for each cuDNN handle.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/dropout.jl#L1-L17">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnDropoutState" href="#cuDNN.cudnnDropoutState"><code>cuDNN.cudnnDropoutState</code></a> — <span class="docstring-category">Constant</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnDropoutForward(x; dropout=0.5)
cudnnDropoutForward(x, d::cudnnDropoutDescriptor)
cudnnDropoutForward!(y, x; dropout=0.5)
cudnnDropoutForward!(y, x, d::cudnnDropoutDescriptor)</code></pre><p>Return a new array similar to <code>x</code> where approximately <code>dropout</code> fraction of the values are replaced by a 0, and the rest are scaled by <code>1/(1-dropout)</code>.  Optionally <code>y</code> holds the result and <code>d</code> specifies the operation. <code>y</code> should be similar to <code>x</code> if specified.</p><p>The user can set the global seed <code>cudnnDropoutSeed[]</code> to a positive number to always drop the same values deterministically for debugging. Note that this slows down the operation by about 40x.</p><p>The global constant <code>cudnnDropoutState::Dict</code> holds the random number generator state for each cuDNN handle.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/dropout.jl#L1-L17">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnActivationDescriptor" href="#cuDNN.cudnnActivationDescriptor"><code>cuDNN.cudnnActivationDescriptor</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnActivationDescriptor(mode::cudnnActivationMode_t,
                          reluNanOpt::cudnnNanPropagation_t,
                          coef::Cfloat)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/descriptors.jl#L53-L57">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnAttnDescriptor" href="#cuDNN.cudnnAttnDescriptor"><code>cuDNN.cudnnAttnDescriptor</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnAttnDescriptor(attnMode::Cuint,
                    nHeads::Cint,
                    smScaler::Cdouble,
                    dataType::cudnnDataType_t,
                    computePrec::cudnnDataType_t,
                    mathType::cudnnMathType_t,
                    attnDropoutDesc::cudnnDropoutDescriptor_t,
                    postDropoutDesc::cudnnDropoutDescriptor_t,
                    qSize::Cint,
                    kSize::Cint,
                    vSize::Cint,
                    qProjSize::Cint,
                    kProjSize::Cint,
                    vProjSize::Cint,
                    oProjSize::Cint,
                    qoMaxSeqLength::Cint,
                    kvMaxSeqLength::Cint,
                    maxBatchSize::Cint,
                    maxBeamSize::Cint)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/descriptors.jl#L61-L81">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnCTCLossDescriptor" href="#cuDNN.cudnnCTCLossDescriptor"><code>cuDNN.cudnnCTCLossDescriptor</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnCTCLossDescriptor(compType::cudnnDataType_t,
                       normMode::cudnnLossNormalizationMode_t,
                       gradMode::cudnnNanPropagation_t,
                       maxLabelLength::Cint)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/descriptors.jl#L85-L90">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnConvolutionDescriptor" href="#cuDNN.cudnnConvolutionDescriptor"><code>cuDNN.cudnnConvolutionDescriptor</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>cudnnConvolutionDescriptor(pad::Vector{Cint},                            stride::Vector{Cint},                            dilation::Vector{Cint},                            mode::cudnnConvolutionMode<em>t,                            dataType::cudnnDataType</em>t,                            groupCount::Cint,                            mathType::cudnnMathType<em>t,                            reorderType::cudnnReorderType</em>t)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/descriptors.jl#L94-L103">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnDropoutDescriptor" href="#cuDNN.cudnnDropoutDescriptor"><code>cuDNN.cudnnDropoutDescriptor</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnDropoutDescriptor(dropout::Real)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/descriptors.jl#L107-L109">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnFilterDescriptor" href="#cuDNN.cudnnFilterDescriptor"><code>cuDNN.cudnnFilterDescriptor</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnFilterDescriptor(dataType::cudnnDataType_t,
                      format::cudnnTensorFormat_t,
                      nbDims::Cint,
                      filterDimA::Vector{Cint})</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/descriptors.jl#L113-L118">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnLRNDescriptor" href="#cuDNN.cudnnLRNDescriptor"><code>cuDNN.cudnnLRNDescriptor</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnLRNDescriptor(lrnN::Cuint,
                   lrnAlpha::Cdouble,
                   lrnBeta::Cdouble,
                   lrnK::Cdouble)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/descriptors.jl#L122-L127">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnOpTensorDescriptor" href="#cuDNN.cudnnOpTensorDescriptor"><code>cuDNN.cudnnOpTensorDescriptor</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnOpTensorDescriptor(opTensorOp::cudnnOpTensorOp_t,
                        opTensorCompType::cudnnDataType_t,
                        opTensorNanOpt::cudnnNanPropagation_t)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/descriptors.jl#L131-L135">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnPoolingDescriptor" href="#cuDNN.cudnnPoolingDescriptor"><code>cuDNN.cudnnPoolingDescriptor</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnPoolingDescriptor(mode::cudnnPoolingMode_t,
                       maxpoolingNanOpt::cudnnNanPropagation_t,
                       nbDims::Cint,
                       windowDimA::Vector{Cint},
                       paddingA::Vector{Cint},
                       strideA::Vector{Cint})</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/descriptors.jl#L139-L146">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnRNNDataDescriptor" href="#cuDNN.cudnnRNNDataDescriptor"><code>cuDNN.cudnnRNNDataDescriptor</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnRNNDataDescriptor(dataType::cudnnDataType_t,
                       layout::cudnnRNNDataLayout_t,
                       maxSeqLength::Cint,
                       batchSize::Cint,
                       vectorSize::Cint,
                       seqLengthArray::Vector{Cint},
                       paddingFill::Ptr{Cvoid})</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/descriptors.jl#L169-L177">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnRNNDescriptor" href="#cuDNN.cudnnRNNDescriptor"><code>cuDNN.cudnnRNNDescriptor</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnRNNDescriptor(algo::cudnnRNNAlgo_t,
                   cellMode::cudnnRNNMode_t,
                   biasMode::cudnnRNNBiasMode_t,
                   dirMode::cudnnDirectionMode_t,
                   inputMode::cudnnRNNInputMode_t,
                   dataType::cudnnDataType_t,
                   mathPrec::cudnnDataType_t,
                   mathType::cudnnMathType_t,
                   inputSize::Int32,
                   hiddenSize::Int32,
                   projSize::Int32,
                   numLayers::Int32,
                   dropoutDesc::cudnnDropoutDescriptor_t,
                   auxFlags::UInt32)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/descriptors.jl#L150-L165">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnReduceTensorDescriptor" href="#cuDNN.cudnnReduceTensorDescriptor"><code>cuDNN.cudnnReduceTensorDescriptor</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnReduceTensorDescriptor(reduceTensorOp::cudnnReduceTensorOp_t,
                            reduceTensorCompType::cudnnDataType_t,
                            reduceTensorNanOpt::cudnnNanPropagation_t,
                            reduceTensorIndices::cudnnReduceTensorIndices_t,
                            reduceTensorIndicesType::cudnnIndicesType_t)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/descriptors.jl#L181-L187">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnSeqDataDescriptor" href="#cuDNN.cudnnSeqDataDescriptor"><code>cuDNN.cudnnSeqDataDescriptor</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnSeqDataDescriptor(dataType::cudnnDataType_t,
                       nbDims::Cint,
                       dimA::Vector{Cint},
                       axes::Vector{cudnnSeqDataAxis_t},
                       seqLengthArraySize::Csize_t,
                       seqLengthArray::Vector{Cint},
                       paddingFill::Ptr{Cvoid})</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/descriptors.jl#L191-L199">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnSpatialTransformerDescriptor" href="#cuDNN.cudnnSpatialTransformerDescriptor"><code>cuDNN.cudnnSpatialTransformerDescriptor</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnSpatialTransformerDescriptor(samplerType::cudnnSamplerType_t,
                                  dataType::cudnnDataType_t,
                                  nbDims::Cint,
                                  dimA::Vector{Cint})</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/descriptors.jl#L203-L208">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnTensorDescriptor" href="#cuDNN.cudnnTensorDescriptor"><code>cuDNN.cudnnTensorDescriptor</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnTensorDescriptor(format::cudnnTensorFormat_t,
                      dataType::cudnnDataType_t,
                      nbDims::Cint,
                      dimA::Vector{Cint})</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/descriptors.jl#L212-L217">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnTensorTransformDescriptor" href="#cuDNN.cudnnTensorTransformDescriptor"><code>cuDNN.cudnnTensorTransformDescriptor</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnTensorTransformDescriptor(nbDims::UInt32,
                               destFormat::cudnnTensorFormat_t,
                               padBeforeA::Vector{Int32},
                               padAfterA::Vector{Int32},
                               foldA::Vector{UInt32},
                               direction::cudnnFoldingDirection_t)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/descriptors.jl#L221-L228">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnActivationForward" href="#cuDNN.cudnnActivationForward"><code>cuDNN.cudnnActivationForward</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnActivationForward(x; mode, nanOpt, coef, alpha)
cudnnActivationForward(x, d::cudnnActivationDescriptor; alpha)
cudnnActivationForward!(y, x; mode, nanOpt, coef, alpha, beta)
cudnnActivationForward!(y, x, d::cudnnActivationDescriptor; alpha, beta)</code></pre><p>Return the result of the specified elementwise activation operation applied to <code>x</code>. Optionally <code>y</code> holds the result and <code>d</code> specifies the operation. <code>y</code> should be similar to <code>x</code> if specified. Keyword arguments <code>alpha=1, beta=0</code> can be used for scaling, i.e. <code>y .= alpha * op.(x) .+ beta * y</code>.  The following keyword arguments specify the operation if <code>d</code> is not given:</p><ul><li><code>mode = CUDNN_ACTIVATION_RELU</code>: Options are <code>SIGMOID</code>, <code>RELU</code>, <code>TANH</code>, <code>CLIPPED_RELU</code>, <code>ELU</code>, <code>IDENTITY</code></li><li><code>nanOpt = CUDNN_NOT_PROPAGATE_NAN</code>: NaN propagation policy, the other option is <code>CUDNN_PROPAGATE_NAN</code></li><li><code>coef=1</code>: When the activation mode is set to <code>CUDNN_ACTIVATION_CLIPPED_RELU</code>, this input specifies the clipping threshold; and when the activation mode is set to <code>CUDNN_ACTIVATION_ELU</code>, this input specifies the <code>alpha</code> parameter.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/activation.jl#L1-L16">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnActivationForward!" href="#cuDNN.cudnnActivationForward!"><code>cuDNN.cudnnActivationForward!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnActivationForward(x; mode, nanOpt, coef, alpha)
cudnnActivationForward(x, d::cudnnActivationDescriptor; alpha)
cudnnActivationForward!(y, x; mode, nanOpt, coef, alpha, beta)
cudnnActivationForward!(y, x, d::cudnnActivationDescriptor; alpha, beta)</code></pre><p>Return the result of the specified elementwise activation operation applied to <code>x</code>. Optionally <code>y</code> holds the result and <code>d</code> specifies the operation. <code>y</code> should be similar to <code>x</code> if specified. Keyword arguments <code>alpha=1, beta=0</code> can be used for scaling, i.e. <code>y .= alpha * op.(x) .+ beta * y</code>.  The following keyword arguments specify the operation if <code>d</code> is not given:</p><ul><li><code>mode = CUDNN_ACTIVATION_RELU</code>: Options are <code>SIGMOID</code>, <code>RELU</code>, <code>TANH</code>, <code>CLIPPED_RELU</code>, <code>ELU</code>, <code>IDENTITY</code></li><li><code>nanOpt = CUDNN_NOT_PROPAGATE_NAN</code>: NaN propagation policy, the other option is <code>CUDNN_PROPAGATE_NAN</code></li><li><code>coef=1</code>: When the activation mode is set to <code>CUDNN_ACTIVATION_CLIPPED_RELU</code>, this input specifies the clipping threshold; and when the activation mode is set to <code>CUDNN_ACTIVATION_ELU</code>, this input specifies the <code>alpha</code> parameter.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/activation.jl#L1-L16">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnAddTensor" href="#cuDNN.cudnnAddTensor"><code>cuDNN.cudnnAddTensor</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnAddTensor(x, b; alpha)
cudnnAddTensor!(y, x, b; alpha, beta)</code></pre><p>Broadcast-add tensor <code>b</code> to tensor <code>x</code>. <code>alpha=1, beta=1</code> are used for scaling, i.e. <code>y .= alpha * b .+ beta * x</code>.  <code>cudnnAddTensor</code> allocates a new array for the answer, <code>cudnnAddTensor!</code> overwrites <code>y</code>. Does not support all valid broadcasting dimensions.  For more flexible broadcast operations see <code>cudnnOpTensor</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/inplace.jl#L42-L50">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnAddTensor!" href="#cuDNN.cudnnAddTensor!"><code>cuDNN.cudnnAddTensor!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnAddTensor(x, b; alpha)
cudnnAddTensor!(y, x, b; alpha, beta)</code></pre><p>Broadcast-add tensor <code>b</code> to tensor <code>x</code>. <code>alpha=1, beta=1</code> are used for scaling, i.e. <code>y .= alpha * b .+ beta * x</code>.  <code>cudnnAddTensor</code> allocates a new array for the answer, <code>cudnnAddTensor!</code> overwrites <code>y</code>. Does not support all valid broadcasting dimensions.  For more flexible broadcast operations see <code>cudnnOpTensor</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/inplace.jl#L42-L50">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnConvolutionBwdDataAlgoPerf" href="#cuDNN.cudnnConvolutionBwdDataAlgoPerf"><code>cuDNN.cudnnConvolutionBwdDataAlgoPerf</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnConvolutionBwdDataAlgoPerf(wDesc, w, dyDesc, dy, convDesc, dxDesc, dx, allocateTmpBuf=true)</code></pre><p><code>allocateTmpBuf</code> controls whether a temporary buffer is allocated for the input gradient dx. It can be set to false when beta is zero to save an allocation and must otherwise be set to true.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/convolution.jl#L234-L239">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnConvolutionBwdFilterAlgoPerf" href="#cuDNN.cudnnConvolutionBwdFilterAlgoPerf"><code>cuDNN.cudnnConvolutionBwdFilterAlgoPerf</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnConvolutionBwdFilterAlgoPerf(xDesc, x, dyDesc, dy, convDesc, dwDesc, dw, allocateTmpBuf=true)</code></pre><p><code>allocateTmpBuf</code> controls whether a temporary buffer is allocated for the weight gradient dw. It can be set to false when beta is zero to save an allocation and must otherwise be set to true.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/convolution.jl#L269-L274">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnConvolutionForward" href="#cuDNN.cudnnConvolutionForward"><code>cuDNN.cudnnConvolutionForward</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnConvolutionForward(w, x; bias, activation, mode, padding, stride, dilation, group, mathType, reorderType, alpha, beta, z, format)
cudnnConvolutionForward(w, x, d::cudnnConvolutionDescriptor; bias, activation, alpha, beta, z, format)
cudnnConvolutionForward!(y, w, x; bias, activation, mode, padding, stride, dilation, group, mathType, reorderType, alpha, beta, z, format)
cudnnConvolutionForward!(y, w, x, d::cudnnConvolutionDescriptor; bias, activation, alpha, beta, z, format)</code></pre><p>Return the convolution of filter <code>w</code> with tensor <code>x</code>, overwriting <code>y</code> if provided, according to keyword arguments or the convolution descriptor <code>d</code>. Optionally perform bias addition, activation and/or scaling:</p><pre><code class="nohighlight hljs">y .= activation.(alpha * conv(w,x) + beta * z .+ bias)</code></pre><p>All tensors should have the same number of dimensions. If they are less than 4-D their dimensions are assumed to be padded on the left with ones. <code>x</code> has size <code>(X...,Cx,N)</code> where <code>(X...)</code> are the spatial dimensions, <code>Cx</code> is the number of input channels, and <code>N</code> is the number of instances. <code>y,z</code> have size <code>(Y...,Cy,N)</code> where <code>(Y...)</code> are the spatial dimensions and <code>Cy</code> is the number of output channels (<code>y</code> and <code>z</code> can be the same array). Both <code>Cx</code> and <code>Cy</code> have to be an exact multiple of <code>group</code>.  <code>w</code> has size <code>(W...,Cx÷group,Cy)</code> where <code>(W...)</code> are the filter dimensions. <code>bias</code> has size <code>(1...,Cy,1)</code>.</p><p>The arguments <code>padding</code>, <code>stride</code> and <code>dilation</code> can be specified as <code>n-2</code> dimensional vectors, tuples or a single integer which is assumed to be repeated <code>n-2</code> times. If any of the entries is larger than the corresponding <code>x</code> dimension, the <code>x</code> dimension is used instead. For a description of different types of convolution see: https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215</p><p>Keyword arguments:</p><ul><li><code>activation = CUDNN_ACTIVATION_IDENTITY</code>: the only other supported option is <code>CUDNN_ACTIVATION_RELU</code></li><li><code>bias = nothing</code>: add bias if provided</li><li><code>z = nothing</code>: add <code>beta*z</code>, <code>z</code> can be <code>nothing</code>, <code>y</code> or another array similar to <code>y</code></li><li><code>alpha = 1, beta = 0</code>: scaling parameters</li><li><code>format = CUDNN_TENSOR_NCHW</code>: order of tensor dimensions, the other alternative is <code>CUDNN_TENSOR_NHWC</code>. Note that Julia dimensions will have the opposite order, i.e. WHCN or CWHN.</li></ul><p>Keyword arguments describing the convolution when <code>d</code> is not given:</p><ul><li><code>mode = CUDNN_CONVOLUTION</code>: alternatively <code>CUDNN_CROSS_CORRELATION</code></li><li><code>padding = 0</code>: padding assumed around <code>x</code></li><li><code>stride = 1</code>: how far to shift the convolution window at each step</li><li><code>dilation = 1</code>: dilation factor</li><li><code>group = 1</code>: number of groups to be used</li><li><code>mathType = cuDNN.math_mode()</code>: whether or not the use of tensor op is permitted</li><li><code>reorderType = CUDNN_DEFAULT_REORDER</code>: convolution reorder type</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/convolution.jl#L4-L45">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnConvolutionForward!" href="#cuDNN.cudnnConvolutionForward!"><code>cuDNN.cudnnConvolutionForward!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnConvolutionForward(w, x; bias, activation, mode, padding, stride, dilation, group, mathType, reorderType, alpha, beta, z, format)
cudnnConvolutionForward(w, x, d::cudnnConvolutionDescriptor; bias, activation, alpha, beta, z, format)
cudnnConvolutionForward!(y, w, x; bias, activation, mode, padding, stride, dilation, group, mathType, reorderType, alpha, beta, z, format)
cudnnConvolutionForward!(y, w, x, d::cudnnConvolutionDescriptor; bias, activation, alpha, beta, z, format)</code></pre><p>Return the convolution of filter <code>w</code> with tensor <code>x</code>, overwriting <code>y</code> if provided, according to keyword arguments or the convolution descriptor <code>d</code>. Optionally perform bias addition, activation and/or scaling:</p><pre><code class="nohighlight hljs">y .= activation.(alpha * conv(w,x) + beta * z .+ bias)</code></pre><p>All tensors should have the same number of dimensions. If they are less than 4-D their dimensions are assumed to be padded on the left with ones. <code>x</code> has size <code>(X...,Cx,N)</code> where <code>(X...)</code> are the spatial dimensions, <code>Cx</code> is the number of input channels, and <code>N</code> is the number of instances. <code>y,z</code> have size <code>(Y...,Cy,N)</code> where <code>(Y...)</code> are the spatial dimensions and <code>Cy</code> is the number of output channels (<code>y</code> and <code>z</code> can be the same array). Both <code>Cx</code> and <code>Cy</code> have to be an exact multiple of <code>group</code>.  <code>w</code> has size <code>(W...,Cx÷group,Cy)</code> where <code>(W...)</code> are the filter dimensions. <code>bias</code> has size <code>(1...,Cy,1)</code>.</p><p>The arguments <code>padding</code>, <code>stride</code> and <code>dilation</code> can be specified as <code>n-2</code> dimensional vectors, tuples or a single integer which is assumed to be repeated <code>n-2</code> times. If any of the entries is larger than the corresponding <code>x</code> dimension, the <code>x</code> dimension is used instead. For a description of different types of convolution see: https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215</p><p>Keyword arguments:</p><ul><li><code>activation = CUDNN_ACTIVATION_IDENTITY</code>: the only other supported option is <code>CUDNN_ACTIVATION_RELU</code></li><li><code>bias = nothing</code>: add bias if provided</li><li><code>z = nothing</code>: add <code>beta*z</code>, <code>z</code> can be <code>nothing</code>, <code>y</code> or another array similar to <code>y</code></li><li><code>alpha = 1, beta = 0</code>: scaling parameters</li><li><code>format = CUDNN_TENSOR_NCHW</code>: order of tensor dimensions, the other alternative is <code>CUDNN_TENSOR_NHWC</code>. Note that Julia dimensions will have the opposite order, i.e. WHCN or CWHN.</li></ul><p>Keyword arguments describing the convolution when <code>d</code> is not given:</p><ul><li><code>mode = CUDNN_CONVOLUTION</code>: alternatively <code>CUDNN_CROSS_CORRELATION</code></li><li><code>padding = 0</code>: padding assumed around <code>x</code></li><li><code>stride = 1</code>: how far to shift the convolution window at each step</li><li><code>dilation = 1</code>: dilation factor</li><li><code>group = 1</code>: number of groups to be used</li><li><code>mathType = cuDNN.math_mode()</code>: whether or not the use of tensor op is permitted</li><li><code>reorderType = CUDNN_DEFAULT_REORDER</code>: convolution reorder type</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/convolution.jl#L4-L45">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnConvolutionFwdAlgoPerf" href="#cuDNN.cudnnConvolutionFwdAlgoPerf"><code>cuDNN.cudnnConvolutionFwdAlgoPerf</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnConvolutionFwdAlgoPerf(xDesc, x, wDesc, w, convDesc, yDesc, y, biasDesc, activation, allocateTmpBuf=true)</code></pre><p><code>allocateTmpBuf</code> controls whether a temporary buffer is allocated for the output y. It can be set to false when beta is zero to save an allocation and must otherwise be set to true.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/convolution.jl#L197-L202">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnDropoutForward" href="#cuDNN.cudnnDropoutForward"><code>cuDNN.cudnnDropoutForward</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnDropoutForward(x; dropout=0.5)
cudnnDropoutForward(x, d::cudnnDropoutDescriptor)
cudnnDropoutForward!(y, x; dropout=0.5)
cudnnDropoutForward!(y, x, d::cudnnDropoutDescriptor)</code></pre><p>Return a new array similar to <code>x</code> where approximately <code>dropout</code> fraction of the values are replaced by a 0, and the rest are scaled by <code>1/(1-dropout)</code>.  Optionally <code>y</code> holds the result and <code>d</code> specifies the operation. <code>y</code> should be similar to <code>x</code> if specified.</p><p>The user can set the global seed <code>cudnnDropoutSeed[]</code> to a positive number to always drop the same values deterministically for debugging. Note that this slows down the operation by about 40x.</p><p>The global constant <code>cudnnDropoutState::Dict</code> holds the random number generator state for each cuDNN handle.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/dropout.jl#L1-L17">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnDropoutForward!" href="#cuDNN.cudnnDropoutForward!"><code>cuDNN.cudnnDropoutForward!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnDropoutForward(x; dropout=0.5)
cudnnDropoutForward(x, d::cudnnDropoutDescriptor)
cudnnDropoutForward!(y, x; dropout=0.5)
cudnnDropoutForward!(y, x, d::cudnnDropoutDescriptor)</code></pre><p>Return a new array similar to <code>x</code> where approximately <code>dropout</code> fraction of the values are replaced by a 0, and the rest are scaled by <code>1/(1-dropout)</code>.  Optionally <code>y</code> holds the result and <code>d</code> specifies the operation. <code>y</code> should be similar to <code>x</code> if specified.</p><p>The user can set the global seed <code>cudnnDropoutSeed[]</code> to a positive number to always drop the same values deterministically for debugging. Note that this slows down the operation by about 40x.</p><p>The global constant <code>cudnnDropoutState::Dict</code> holds the random number generator state for each cuDNN handle.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/dropout.jl#L1-L17">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnGetRNNWeightParams-Tuple{Any}" href="#cuDNN.cudnnGetRNNWeightParams-Tuple{Any}"><code>cuDNN.cudnnGetRNNWeightParams</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnGetRNNWeightParams(w, d::cudnnRNNDescriptor)
cudnnGetRNNWeightParams(w; hiddenSize, o...)</code></pre><p>Return an array of weight matrices and bias vectors of an RNN specified by <code>d</code> or keyword options as views into <code>w</code>. The keyword arguments and defaults in the second form are the same as those in cudnnRNNForward specifying the RNN.</p><p>In the returned array <code>a[1,l,p]</code> and <code>a[2,l,p]</code> give the weight matrix and bias vector for the l&#39;th layer and p&#39;th parameter or <code>nothing</code> if the specified matrix/vector does not exist. Note that the matrices should be transposed for left multiplication, e.g. `a[1,l,p]&#39;</p><ul><li>x`</li></ul><p>The <code>l</code> index refers to the pseudo-layer number. In uni-directional RNNs, a pseudo-layer is the same as a physical layer (pseudoLayer=1 is the RNN input layer, pseudoLayer=2 is the first hidden layer). In bi-directional RNNs, there are twice as many pseudo-layers in comparison to physical layers:</p><pre><code class="nohighlight hljs">pseudoLayer=1 refers to the forward direction sub-layer of the physical input layer
pseudoLayer=2 refers to the backward direction sub-layer of the physical input layer
pseudoLayer=3 is the forward direction sub-layer of the first hidden layer, and so on</code></pre><p>The <code>p</code> index refers to the weight matrix or bias vector linear ID index.</p><p>If cellMode in rnnDesc was set to CUDNN<em>RNN</em>RELU or CUDNN<em>RNN</em>TANH:</p><pre><code class="nohighlight hljs">Value 1 references the weight matrix or bias vector used in conjunction with the input from the previous layer or input to the RNN model.
Value 2 references the weight matrix or bias vector used in conjunction with the hidden state from the previous time step or the initial hidden state.</code></pre><p>If cellMode in rnnDesc was set to CUDNN_LSTM:</p><pre><code class="nohighlight hljs">Values 1, 2, 3 and 4 reference weight matrices or bias vectors used in conjunction with the input from the previous layer or input to the RNN model.
Values 5, 6, 7 and 8 reference weight matrices or bias vectors used in conjunction with the hidden state from the previous time step or the initial hidden state.
Value 9 corresponds to the projection matrix, if enabled (there is no bias in this operation).</code></pre><p>Values and their LSTM gates:</p><pre><code class="nohighlight hljs">Values 1 and 5 correspond to the input gate.
Values 2 and 6 correspond to the forget gate.
Values 3 and 7 correspond to the new cell state calculations with hyperbolic tangent.
Values 4 and 8 correspond to the output gate.</code></pre><p>If cellMode in rnnDesc was set to CUDNN_GRU:</p><pre><code class="nohighlight hljs">Values 1, 2 and 3 reference weight matrices or bias vectors used in conjunction with the input from the previous layer or input to the RNN model.
Values 4, 5 and 6 reference weight matrices or bias vectors used in conjunction with the hidden state from the previous time step or the initial hidden state.</code></pre><p>Values and their GRU gates:</p><pre><code class="nohighlight hljs">Values 1 and 4 correspond to the reset gate.
Values 2 and 5 reference to the update gate.
Values 3 and 6 correspond to the new hidden state calculations with hyperbolic tangent.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/rnn.jl#L215-L268">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnMultiHeadAttnForward" href="#cuDNN.cudnnMultiHeadAttnForward"><code>cuDNN.cudnnMultiHeadAttnForward</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnMultiHeadAttnForward(weights, queries, keys, values; o...)
cudnnMultiHeadAttnForward!(out, weights, queries, keys, values; o...)
cudnnMultiHeadAttnForward(weights, queries, keys, values, d::cudnnAttnDescriptor; o...)
cudnnMultiHeadAttnForward!(out, weights, queries, keys, values, d::cudnnAttnDescriptor; o...)</code></pre><p>Return the multi-head attention result with <code>weights</code>, <code>queries</code>, <code>keys</code>, and <code>values</code>, overwriting <code>out</code> if provided, according to keyword arguments or the attention descriptor <code>d</code>.  The multi-head attention model can be described by the following equations:</p><p class="math-container">\[\begin{aligned}
&amp;h_i = (W_{V,i} V) \operatorname{softmax}(\operatorname{smScaler}(K^T W^T_{K,i}) (W_{Q,i} q))
&amp;\operatorname(MultiHeadAttn)(q,K,V,W_Q,W_K,W_V,W_O) = \sum_{i=1}^{\operatorname{nHeads}-1} W_{O,i} h_i
\end{aligned}\]</p><p>The input arguments are:</p><ul><li><code>out</code>: Optional output tensor.</li><li><code>weights</code>: A weight buffer that contains <span>$W_Q, W_K, W_V, W_O$</span>.</li><li><code>queries</code>: A query tensor <span>$Q$</span> which may contain a batch of queries (the above equations were for a single query vector <span>$q$</span> for simplicity).</li><li><code>keys</code>: The keys tensor <span>$K$</span>.</li><li><code>values</code>: The values tensor <span>$V$</span>.</li></ul><p>Keyword arguments describing the tensors:</p><ul><li><code>axes::Vector{cudnnSeqDataAxis_t} = [CUDNN_SEQDATA_VECT_DIM, CUDNN_SEQDATA_BATCH_DIM, CUDNN_SEQDATA_TIME_DIM, CUDNN_SEQDATA_BEAM_DIM]</code>: an array of length 4 that specifies the role of (Julia) dimensions. VECT has to be the first dimension, all 6 permutations of the remaining three are supported.</li><li><code>seqLengthsQO::Vector{&lt;:Integer}</code>: sequence lengths in the queries and out containers. By default sequences are assumed to be full length of the TIME dimension.</li><li><code>seqLengthsKV::Vector{&lt;:Integer}</code>: sequence lengths in the keys and values containers. By default sequences are assumed to be full length of the TIME dimension.</li></ul><p>Keyword arguments describing the attention operation when <code>d</code> is not given:</p><ul><li><code>attnMode::Unsigned = CUDNN_ATTN_QUERYMAP_ALL_TO_ONE | CUDNN_ATTN_DISABLE_PROJ_BIASES</code>: bitwise flags indicating various attention options. See cudnn docs for details.</li><li><code>nHeads::Integer = 1</code>: number of attention heads.</li><li><code>smScaler::Real = 1</code>: softmax smoothing (<code>1.0 &gt;= smScaler &gt;= 0.0</code>) or sharpening (<code>smScaler &gt; 1.0</code>) coefficient. Negative values are not accepted.</li><li><code>mathType::cudnnMathType_t = math_mode()</code>: NVIDIA Tensor Core settings.</li><li><code>qProjSize, kProjSize, vProjSize, oProjSize</code>: vector lengths after projections, set to 0 by default which disables projections.</li><li><code>qoMaxSeqLength::Integer</code>: largest sequence length expected in queries and out, set to their TIME dim by default.</li><li><code>kvMaxSeqLength::Integer</code>: largest sequence length expected in keys and values, set to their TIME dim by default.</li><li><code>maxBatchSize::Integer</code>: largest batch size expected in any container, set to the BATCH dim of queries by default.</li><li><code>maxBeamSize::Integer</code>: largest beam size expected in any container, set to the BEAM dim of queries by default.</li></ul><p>Other keyword arguments:</p><ul><li><code>residuals = nothing</code>: optional tensor with the same size as queries that can be used to implement residual connections (see figure in cudnn docs). When residual connections are enabled, the vector length in <code>queries</code> should match the vector length in <code>out</code>, so that a vector addition is feasible. </li><li><code>currIdx::Integer = -1</code>: Time-step (0-based) in queries to process. When the <code>currIdx</code> argument is negative, all <span>$Q$</span> time-steps are processed. When <code>currIdx</code> is zero or positive, the forward response is computed for the selected time-step only. The latter input can be used in inference mode only, to process one time-step while updating the next attention window and <span>$Q$</span>, <span>$K$</span>, <span>$V$</span> inputs in-between calls.</li><li><code>loWinIdx, hiWinIdx::Array{Cint}</code>: Two host integer arrays specifying the start and end (0-based) indices of the attention window for each <span>$Q$</span> time-step. The start index in <span>$K$</span>, <span>$V$</span> sets is inclusive, and the end index is exclusive. By default set at 0 and <code>kvMaxSeqLength</code> respectively.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/multiheadattn.jl#L1-L45">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnMultiHeadAttnForward!" href="#cuDNN.cudnnMultiHeadAttnForward!"><code>cuDNN.cudnnMultiHeadAttnForward!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnMultiHeadAttnForward(weights, queries, keys, values; o...)
cudnnMultiHeadAttnForward!(out, weights, queries, keys, values; o...)
cudnnMultiHeadAttnForward(weights, queries, keys, values, d::cudnnAttnDescriptor; o...)
cudnnMultiHeadAttnForward!(out, weights, queries, keys, values, d::cudnnAttnDescriptor; o...)</code></pre><p>Return the multi-head attention result with <code>weights</code>, <code>queries</code>, <code>keys</code>, and <code>values</code>, overwriting <code>out</code> if provided, according to keyword arguments or the attention descriptor <code>d</code>.  The multi-head attention model can be described by the following equations:</p><p class="math-container">\[\begin{aligned}
&amp;h_i = (W_{V,i} V) \operatorname{softmax}(\operatorname{smScaler}(K^T W^T_{K,i}) (W_{Q,i} q))
&amp;\operatorname(MultiHeadAttn)(q,K,V,W_Q,W_K,W_V,W_O) = \sum_{i=1}^{\operatorname{nHeads}-1} W_{O,i} h_i
\end{aligned}\]</p><p>The input arguments are:</p><ul><li><code>out</code>: Optional output tensor.</li><li><code>weights</code>: A weight buffer that contains <span>$W_Q, W_K, W_V, W_O$</span>.</li><li><code>queries</code>: A query tensor <span>$Q$</span> which may contain a batch of queries (the above equations were for a single query vector <span>$q$</span> for simplicity).</li><li><code>keys</code>: The keys tensor <span>$K$</span>.</li><li><code>values</code>: The values tensor <span>$V$</span>.</li></ul><p>Keyword arguments describing the tensors:</p><ul><li><code>axes::Vector{cudnnSeqDataAxis_t} = [CUDNN_SEQDATA_VECT_DIM, CUDNN_SEQDATA_BATCH_DIM, CUDNN_SEQDATA_TIME_DIM, CUDNN_SEQDATA_BEAM_DIM]</code>: an array of length 4 that specifies the role of (Julia) dimensions. VECT has to be the first dimension, all 6 permutations of the remaining three are supported.</li><li><code>seqLengthsQO::Vector{&lt;:Integer}</code>: sequence lengths in the queries and out containers. By default sequences are assumed to be full length of the TIME dimension.</li><li><code>seqLengthsKV::Vector{&lt;:Integer}</code>: sequence lengths in the keys and values containers. By default sequences are assumed to be full length of the TIME dimension.</li></ul><p>Keyword arguments describing the attention operation when <code>d</code> is not given:</p><ul><li><code>attnMode::Unsigned = CUDNN_ATTN_QUERYMAP_ALL_TO_ONE | CUDNN_ATTN_DISABLE_PROJ_BIASES</code>: bitwise flags indicating various attention options. See cudnn docs for details.</li><li><code>nHeads::Integer = 1</code>: number of attention heads.</li><li><code>smScaler::Real = 1</code>: softmax smoothing (<code>1.0 &gt;= smScaler &gt;= 0.0</code>) or sharpening (<code>smScaler &gt; 1.0</code>) coefficient. Negative values are not accepted.</li><li><code>mathType::cudnnMathType_t = math_mode()</code>: NVIDIA Tensor Core settings.</li><li><code>qProjSize, kProjSize, vProjSize, oProjSize</code>: vector lengths after projections, set to 0 by default which disables projections.</li><li><code>qoMaxSeqLength::Integer</code>: largest sequence length expected in queries and out, set to their TIME dim by default.</li><li><code>kvMaxSeqLength::Integer</code>: largest sequence length expected in keys and values, set to their TIME dim by default.</li><li><code>maxBatchSize::Integer</code>: largest batch size expected in any container, set to the BATCH dim of queries by default.</li><li><code>maxBeamSize::Integer</code>: largest beam size expected in any container, set to the BEAM dim of queries by default.</li></ul><p>Other keyword arguments:</p><ul><li><code>residuals = nothing</code>: optional tensor with the same size as queries that can be used to implement residual connections (see figure in cudnn docs). When residual connections are enabled, the vector length in <code>queries</code> should match the vector length in <code>out</code>, so that a vector addition is feasible. </li><li><code>currIdx::Integer = -1</code>: Time-step (0-based) in queries to process. When the <code>currIdx</code> argument is negative, all <span>$Q$</span> time-steps are processed. When <code>currIdx</code> is zero or positive, the forward response is computed for the selected time-step only. The latter input can be used in inference mode only, to process one time-step while updating the next attention window and <span>$Q$</span>, <span>$K$</span>, <span>$V$</span> inputs in-between calls.</li><li><code>loWinIdx, hiWinIdx::Array{Cint}</code>: Two host integer arrays specifying the start and end (0-based) indices of the attention window for each <span>$Q$</span> time-step. The start index in <span>$K$</span>, <span>$V$</span> sets is inclusive, and the end index is exclusive. By default set at 0 and <code>kvMaxSeqLength</code> respectively.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/multiheadattn.jl#L1-L45">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnNormalizationForward" href="#cuDNN.cudnnNormalizationForward"><code>cuDNN.cudnnNormalizationForward</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnNormalizationForward(x, xmean, xvar, bias, scale; o...)
cudnnNormalizationForward!(y, x, xmean, xvar, bias, scale; o...)</code></pre><p>Return batch normalization applied to <code>x</code>:</p><pre><code class="nohighlight hljs">y .= ((x .- mean(x; dims)) ./ sqrt.(epsilon .+ var(x; dims))) .* scale .+ bias  # training
y .= ((x .- xmean) ./ sqrt.(epsilon .+ xvar)) .* scale .+ bias                  # inference</code></pre><p><code>bias</code> and <code>scale</code> are trainable parameters, <code>xmean</code> and <code>xvar</code> are modified to collect statistics during training and treated as constants during inference. Note that during inference the values given by <code>xmean</code> and <code>xvar</code> arguments are used in the formula whereas during training the actual mean and variance of the minibatch are used in the formula: the <code>xmean</code>/<code>xvar</code> arguments are only used to collect statistics. In the original paper <code>bias</code> is referred to as <code>beta</code> and <code>scale</code> as <code>gamma</code> (Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, S. Ioffe, C. Szegedy, 2015).</p><p>Keyword arguments:</p><ul><li><code>epsilon = 1e-5</code>: epsilon value used in the normalization formula</li><li><code>exponentialAverageFactor = 0.1</code>: factor used in running mean/variance calculation: <code>runningMean = runningMean*(1-factor) + newMean*factor</code></li><li><code>training = false</code>: boolean indicating training vs inference mode</li><li><code>mode::cudnnNormMode_t = CUDNN_NORM_PER_CHANNEL</code>: Per-channel layer is based on the paper. In this mode <code>scale</code> etc. have dimensions <code>(1,1,C,1)</code>. The other alternative is <code>CUDNN_NORM_PER_ACTIVATION</code> where <code>scale</code> etc. have dimensions <code>(W,H,C,1)</code>.</li><li><code>algo::cudnnNormAlgo_t = CUDNN_NORM_ALGO_STANDARD</code>: The other alternative, <code>CUDNN_NORM_ALGO_PERSIST</code>, triggers the new semi-persistent NHWC kernel when certain conditions are met (see cudnn docs).</li><li><code>normOps::cudnnNormOps_t = CUDNN_NORM_OPS_NORM</code>: Currently the other alternatives, <code>CUDNN_NORM_OPS_NORM_ACTIVATION</code> and <code>CUDNN_NORM_OPS_NORM_ADD_ACTIVATION</code> are not supported.</li><li><code>z = nothing</code>: for residual addition to the result of the normalization operation, prior to the activation (will be supported when <code>CUDNN_NORM_OPS_NORM_ADD_ACTIVATION</code> is supported)</li><li><code>groupCnt = 1</code>: Place holder for future work, should be set to 1 now</li><li><code>alpha = 1; beta = 0</code>: scaling parameters: return <code>alpha * new_y + beta * old_y</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/normalization.jl#L1-L30">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnNormalizationForward!" href="#cuDNN.cudnnNormalizationForward!"><code>cuDNN.cudnnNormalizationForward!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnNormalizationForward(x, xmean, xvar, bias, scale; o...)
cudnnNormalizationForward!(y, x, xmean, xvar, bias, scale; o...)</code></pre><p>Return batch normalization applied to <code>x</code>:</p><pre><code class="nohighlight hljs">y .= ((x .- mean(x; dims)) ./ sqrt.(epsilon .+ var(x; dims))) .* scale .+ bias  # training
y .= ((x .- xmean) ./ sqrt.(epsilon .+ xvar)) .* scale .+ bias                  # inference</code></pre><p><code>bias</code> and <code>scale</code> are trainable parameters, <code>xmean</code> and <code>xvar</code> are modified to collect statistics during training and treated as constants during inference. Note that during inference the values given by <code>xmean</code> and <code>xvar</code> arguments are used in the formula whereas during training the actual mean and variance of the minibatch are used in the formula: the <code>xmean</code>/<code>xvar</code> arguments are only used to collect statistics. In the original paper <code>bias</code> is referred to as <code>beta</code> and <code>scale</code> as <code>gamma</code> (Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, S. Ioffe, C. Szegedy, 2015).</p><p>Keyword arguments:</p><ul><li><code>epsilon = 1e-5</code>: epsilon value used in the normalization formula</li><li><code>exponentialAverageFactor = 0.1</code>: factor used in running mean/variance calculation: <code>runningMean = runningMean*(1-factor) + newMean*factor</code></li><li><code>training = false</code>: boolean indicating training vs inference mode</li><li><code>mode::cudnnNormMode_t = CUDNN_NORM_PER_CHANNEL</code>: Per-channel layer is based on the paper. In this mode <code>scale</code> etc. have dimensions <code>(1,1,C,1)</code>. The other alternative is <code>CUDNN_NORM_PER_ACTIVATION</code> where <code>scale</code> etc. have dimensions <code>(W,H,C,1)</code>.</li><li><code>algo::cudnnNormAlgo_t = CUDNN_NORM_ALGO_STANDARD</code>: The other alternative, <code>CUDNN_NORM_ALGO_PERSIST</code>, triggers the new semi-persistent NHWC kernel when certain conditions are met (see cudnn docs).</li><li><code>normOps::cudnnNormOps_t = CUDNN_NORM_OPS_NORM</code>: Currently the other alternatives, <code>CUDNN_NORM_OPS_NORM_ACTIVATION</code> and <code>CUDNN_NORM_OPS_NORM_ADD_ACTIVATION</code> are not supported.</li><li><code>z = nothing</code>: for residual addition to the result of the normalization operation, prior to the activation (will be supported when <code>CUDNN_NORM_OPS_NORM_ADD_ACTIVATION</code> is supported)</li><li><code>groupCnt = 1</code>: Place holder for future work, should be set to 1 now</li><li><code>alpha = 1; beta = 0</code>: scaling parameters: return <code>alpha * new_y + beta * old_y</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/normalization.jl#L1-L30">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnOpTensor" href="#cuDNN.cudnnOpTensor"><code>cuDNN.cudnnOpTensor</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnOpTensor(x1, x2; op, compType, nanOpt, alpha1, alpha2)
cudnnOpTensor(x1, x2, d::cudnnOpTensorDescriptor; alpha1, alpha2)
cudnnOpTensor!(y, x1, x2; op, compType, nanOpt, alpha1, alpha2, beta)
cudnnOpTensor!(y, x1, x2, d::cudnnOpTensorDescriptor; alpha1, alpha2, beta)</code></pre><p>Return the result of the specified broadcasting operation applied to <code>x1</code> and <code>x2</code>. Optionally <code>y</code> holds the result and <code>d</code> specifies the operation. Each dimension of the input tensor <code>x1</code> must match the corresponding dimension of the destination tensor <code>y</code>, and each dimension of the input tensor <code>x2</code> must match the corresponding dimension of the destination tensor <code>y</code> or must be equal to 1. Keyword arguments:</p><ul><li><code>alpha1=1, alpha2=1, beta=0</code> are used for scaling, i.e. <code>y .= beta*y .+ op.(alpha1*x1, alpha2*x2)</code></li></ul><p>Keyword arguments used when <code>cudnnOpTensorDescriptor</code> is not specified:</p><ul><li><code>op = CUDNN_OP_TENSOR_ADD</code>, <code>ADD</code> can be replaced with <code>MUL</code>, <code>MIN</code>, <code>MAX</code>, <code>SQRT</code>, <code>NOT</code>; <code>SQRT</code> and <code>NOT</code> performed only on <code>x1</code>; <code>NOT</code> computes <code>1-x1</code></li><li><code>compType = (eltype(x1) &lt;: Float64 ? Float64 : Float32)</code>: Computation datatype (see cudnn docs for available options)</li><li><code>nanOpt = CUDNN_NOT_PROPAGATE_NAN</code>: NaN propagation policy. The other option is <code>CUDNN_PROPAGATE_NAN</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/optensor.jl#L5-L24">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnOpTensor!" href="#cuDNN.cudnnOpTensor!"><code>cuDNN.cudnnOpTensor!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnOpTensor(x1, x2; op, compType, nanOpt, alpha1, alpha2)
cudnnOpTensor(x1, x2, d::cudnnOpTensorDescriptor; alpha1, alpha2)
cudnnOpTensor!(y, x1, x2; op, compType, nanOpt, alpha1, alpha2, beta)
cudnnOpTensor!(y, x1, x2, d::cudnnOpTensorDescriptor; alpha1, alpha2, beta)</code></pre><p>Return the result of the specified broadcasting operation applied to <code>x1</code> and <code>x2</code>. Optionally <code>y</code> holds the result and <code>d</code> specifies the operation. Each dimension of the input tensor <code>x1</code> must match the corresponding dimension of the destination tensor <code>y</code>, and each dimension of the input tensor <code>x2</code> must match the corresponding dimension of the destination tensor <code>y</code> or must be equal to 1. Keyword arguments:</p><ul><li><code>alpha1=1, alpha2=1, beta=0</code> are used for scaling, i.e. <code>y .= beta*y .+ op.(alpha1*x1, alpha2*x2)</code></li></ul><p>Keyword arguments used when <code>cudnnOpTensorDescriptor</code> is not specified:</p><ul><li><code>op = CUDNN_OP_TENSOR_ADD</code>, <code>ADD</code> can be replaced with <code>MUL</code>, <code>MIN</code>, <code>MAX</code>, <code>SQRT</code>, <code>NOT</code>; <code>SQRT</code> and <code>NOT</code> performed only on <code>x1</code>; <code>NOT</code> computes <code>1-x1</code></li><li><code>compType = (eltype(x1) &lt;: Float64 ? Float64 : Float32)</code>: Computation datatype (see cudnn docs for available options)</li><li><code>nanOpt = CUDNN_NOT_PROPAGATE_NAN</code>: NaN propagation policy. The other option is <code>CUDNN_PROPAGATE_NAN</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/optensor.jl#L5-L24">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnPoolingForward" href="#cuDNN.cudnnPoolingForward"><code>cuDNN.cudnnPoolingForward</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnPoolingForward(x; mode, nanOpt, window, padding, stride, alpha)
cudnnPoolingForward(x, d::cudnnPoolingDescriptor; alpha)
cudnnPoolingForward!(y, x; mode, nanOpt, window, padding, stride, alpha, beta)
cudnnPoolingForward!(y, x, d::cudnnPoolingDescriptor; alpha, beta)</code></pre><p>Return pooled <code>x</code>, overwriting <code>y</code> if provided, according to keyword arguments or the pooling descriptor <code>d</code>. Please see the <a href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingForward">cuDNN docs</a> for details.</p><p>The dimensions of <code>x,y</code> tensors that are less than 4-D are assumed to be padded on the left with 1&#39;s. The first <code>n-2</code> are spatial dimensions, the last two are always assumed to be channel and batch.</p><p>The arguments <code>window</code>, <code>padding</code>, and <code>stride</code> can be specified as <code>n-2</code> dimensional vectors, tuples or a single integer which is assumed to be repeated <code>n-2</code> times. If any of the entries is larger than the corresponding <code>x</code> dimension, the <code>x</code> dimension is used instead.</p><p>Arguments:</p><ul><li><code>mode = CUDNN_POOLING_MAX</code>: Pooling method, other options are <code>CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING</code>, <code>CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING</code>, <code>CUDNN_POOLING_MAX_DETERMINISTIC</code></li><li><code>nanOpt = CUDNN_NOT_PROPAGATE_NAN</code>: NaN propagation policy, the other option is <code>CUDNN_PROPAGATE_NAN</code></li><li><code>window = 2</code>: Pooling window size</li><li><code>padding = 0</code>: Padding assumed around <code>x</code></li><li><code>stride = window</code>: How far to shift pooling window at each step</li><li><code>alpha=1, beta=0</code> can be used for scaling, i.e. <code>y .= alpha * op(x1) .+ beta * y</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/pooling.jl#L1-L28">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnPoolingForward!" href="#cuDNN.cudnnPoolingForward!"><code>cuDNN.cudnnPoolingForward!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnPoolingForward(x; mode, nanOpt, window, padding, stride, alpha)
cudnnPoolingForward(x, d::cudnnPoolingDescriptor; alpha)
cudnnPoolingForward!(y, x; mode, nanOpt, window, padding, stride, alpha, beta)
cudnnPoolingForward!(y, x, d::cudnnPoolingDescriptor; alpha, beta)</code></pre><p>Return pooled <code>x</code>, overwriting <code>y</code> if provided, according to keyword arguments or the pooling descriptor <code>d</code>. Please see the <a href="https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnPoolingForward">cuDNN docs</a> for details.</p><p>The dimensions of <code>x,y</code> tensors that are less than 4-D are assumed to be padded on the left with 1&#39;s. The first <code>n-2</code> are spatial dimensions, the last two are always assumed to be channel and batch.</p><p>The arguments <code>window</code>, <code>padding</code>, and <code>stride</code> can be specified as <code>n-2</code> dimensional vectors, tuples or a single integer which is assumed to be repeated <code>n-2</code> times. If any of the entries is larger than the corresponding <code>x</code> dimension, the <code>x</code> dimension is used instead.</p><p>Arguments:</p><ul><li><code>mode = CUDNN_POOLING_MAX</code>: Pooling method, other options are <code>CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING</code>, <code>CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING</code>, <code>CUDNN_POOLING_MAX_DETERMINISTIC</code></li><li><code>nanOpt = CUDNN_NOT_PROPAGATE_NAN</code>: NaN propagation policy, the other option is <code>CUDNN_PROPAGATE_NAN</code></li><li><code>window = 2</code>: Pooling window size</li><li><code>padding = 0</code>: Padding assumed around <code>x</code></li><li><code>stride = window</code>: How far to shift pooling window at each step</li><li><code>alpha=1, beta=0</code> can be used for scaling, i.e. <code>y .= alpha * op(x1) .+ beta * y</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/pooling.jl#L1-L28">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnRNNForward" href="#cuDNN.cudnnRNNForward"><code>cuDNN.cudnnRNNForward</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnRNNForward(w, x; hiddenSize, o...)
cudnnRNNForward!(y, w, x; hiddenSize, o...)
cudnnRNNForward(w, x, d::cudnnRNNDescriptor; o...)
cudnnRNNForward!(y, w, x, d::cudnnRNNDescriptor; o...)</code></pre><p>Apply the RNN specified with weights <code>w</code> and configuration given by <code>d</code> or keyword options to input <code>x</code>.</p><p>Keyword arguments for hidden input/output:</p><ul><li><code>hx=nothing</code>: initialize the hidden vector if specified (by default initialized to 0).</li><li><code>cx=nothing</code>: initialize the cell vector (only in LSTMs) if specified (by default initialized to 0).</li><li><code>hy=nothing</code>: return the final hidden vector in <code>hy</code> if set to <code>Ref{Any}()</code>.</li><li><code>cy=nothing</code>: return the final cell vector in <code>cy</code> (only in LSTMs) if set to <code>Ref{Any}()</code>.</li></ul><p>Keyword arguments specifying the RNN when <code>d::cudnnRNNDescriptor</code> is not given:</p><ul><li><code>hiddenSize::Integer</code>: hidden vector size, which must be supplied when <code>d</code> is not given</li><li><code>algo::cudnnRNNAlgo_t = CUDNN_RNN_ALGO_STANDARD</code>: RNN algo (<code>CUDNN_RNN_ALGO_STANDARD</code>, <code>CUDNN_RNN_ALGO_PERSIST_STATIC</code>, or <code>CUDNN_RNN_ALGO_PERSIST_DYNAMIC</code>).</li><li><code>cellMode::cudnnRNNMode_t = CUDNN_LSTM</code>: Specifies the RNN cell type in the entire model (<code>CUDNN_RNN_RELU</code>, <code>CUDNN_RNN_TANH</code>, <code>CUDNN_LSTM</code>, <code>CUDNN_GRU</code>).</li><li><code>biasMode::cudnnRNNBiasMode_t = CUDNN_RNN_DOUBLE_BIAS</code>: Sets the number of bias vectors (<code>CUDNN_RNN_NO_BIAS</code>, <code>CUDNN_RNN_SINGLE_INP_BIAS</code>, <code>CUDNN_RNN_SINGLE_REC_BIAS</code>, <code>CUDNN_RNN_DOUBLE_BIAS</code>). The two single bias settings are functionally the same for <code>RELU</code>, <code>TANH</code> and <code>LSTM</code> cell types. For differences in <code>GRU</code> cells, see the description of <code>CUDNN_GRU</code> in cudnn docs.</li><li><code>dirMode::cudnnDirectionMode_t = CUDNN_UNIDIRECTIONAL</code>: Specifies the recurrence pattern: <code>CUDNN_UNIDIRECTIONAL</code> or <code>CUDNN_BIDIRECTIONAL</code>. In bidirectional RNNs, the hidden states passed between physical layers are concatenations of forward and backward hidden states.</li><li><code>inputMode::cudnnRNNInputMode_t = CUDNN_LINEAR_INPUT</code>: Specifies how the input to the RNN model is processed by the first layer. When <code>inputMode</code> is <code>CUDNN_LINEAR_INPUT</code>, original input vectors of size <code>inputSize</code> are multiplied by the weight matrix to obtain vectors of <code>hiddenSize</code>. When <code>inputMode</code> is <code>CUDNN_SKIP_INPUT</code>, the original input vectors to the first layer are used as is without multiplying them by the weight matrix.</li><li><code>mathPrec::DataType = eltype(x)</code>: This parameter is used to control the compute math precision in the RNN model. For <code>Float16</code> input/output can be <code>Float16</code> or <code>Float32</code>, for <code>Float32</code> or <code>Float64</code> input/output, must match the input/output type.</li><li><code>mathType::cudnnMathType_t = math_mode()</code>: Sets the preferred option to use NVIDIA Tensor Cores accelerators on Volta (SM 7.0) or higher GPUs. When <code>dataType</code> is <code>CUDNN_DATA_HALF</code>, the <code>mathType</code> parameter can be <code>CUDNN_DEFAULT_MATH</code> or <code>CUDNN_TENSOR_OP_MATH</code>. The <code>ALLOW_CONVERSION</code> setting is treated the same <code>CUDNN_TENSOR_OP_MATH</code> for this data type. When <code>dataType</code> is <code>CUDNN_DATA_FLOAT</code>, the <code>mathType</code> parameter can be <code>CUDNN_DEFAULT_MATH</code> or <code>CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</code>. When the latter settings are used, original weights and intermediate results will be down-converted to <code>CUDNN_DATA_HALF</code> before they are used in another recursive iteration. When dataType is <code>CUDNN_DATA_DOUBLE</code>, the <code>mathType</code> parameter can be <code>CUDNN_DEFAULT_MATH</code>.</li><li><code>inputSize::Integer = size(x,1)</code>: Size of the input vector in the RNN model. When the <code>inputMode=CUDNN_SKIP_INPUT</code>, the <code>inputSize</code> should match the <code>hiddenSize</code> value.</li><li><code>projSize::Integer = hiddenSize</code>: The size of the LSTM cell output after the recurrent projection. This value should not be larger than <code>hiddenSize</code>. It is legal to set <code>projSize</code> equal to <code>hiddenSize</code>, however, in this case, the recurrent projection feature is disabled. The recurrent projection is an additional matrix multiplication in the LSTM cell to project hidden state vectors <code>ht</code> into smaller vectors <code>rt = Wr * ht</code>, where <code>Wr</code> is a rectangular matrix with <code>projSize</code> rows and hiddenSize columns. When the recurrent projection is enabled, the output of the LSTM cell (both to the next layer and unrolled in-time) is <code>rt</code> instead of <code>ht</code>. The recurrent projection can be enabled for LSTM cells and <code>CUDNN_RNN_ALGO_STANDARD</code> only.</li><li><code>numLayers::Integer = 1</code>: Number of stacked, physical layers in the deep RNN model. When <code>dirMode= CUDNN_BIDIRECTIONAL</code>, the physical layer consists of two pseudo-layers corresponding to forward and backward directions.</li><li><code>dropout::Real = 0</code>: When non-zero, dropout operation will be applied between physical layers. A single layer network will have no dropout applied. Dropout is used in the training mode only.</li><li><code>auxFlags::Integer = CUDNN_RNN_PADDED_IO_ENABLED</code>: Miscellaneous switches that do not require additional numerical values to configure the corresponding feature. In future cuDNN releases, this parameter will be used to extend the RNN functionality without adding new API functions (applicable options should be bitwise OR-ed). Currently, this parameter is used to enable or disable padded input/output (<code>CUDNN_RNN_PADDED_IO_DISABLED</code>, <code>CUDNN_RNN_PADDED_IO_ENABLED</code>). When the padded I/O is enabled, layouts <code>CUDNN_RNN_DATA_LAYOUT_SEQ_MAJOR_UNPACKED</code> and <code>CUDNN_RNN_DATA_LAYOUT_BATCH_MAJOR_UNPACKED</code> are permitted in RNN data descriptors.</li></ul><p>Other keyword arguments:</p><ul><li><code>layout::cudnnRNNDataLayout_t = CUDNN_RNN_DATA_LAYOUT_SEQ_MAJOR_UNPACKED</code>: The memory layout of the RNN data tensor. Options are <code>CUDNN_RNN_DATA_LAYOUT_SEQ_MAJOR_UNPACKED</code>: Data layout is padded, with outer stride from one time-step to the next; <code>CUDNN_RNN_DATA_LAYOUT_SEQ_MAJOR_PACKED</code>: The sequence length is sorted and packed as in the basic RNN API; <code>CUDNN_RNN_DATA_LAYOUT_BATCH_MAJOR_UNPACKED</code>: Data layout is padded, with outer stride from one batch to the next.</li><li><code>seqLengthArray::Vector{Cint} = nothing</code>: An integer array with <code>batchSize</code> number of elements. Describes the length (number of time-steps) of each sequence. Each element in <code>seqLengthArray</code> must be greater than or equal to 0 but less than or equal to <code>maxSeqLength</code>. In the packed layout, the elements should be sorted in descending order, similar to the layout required by the non-extended RNN compute functions. The default value <code>nothing</code> assumes uniform <code>seqLength</code>s, no padding.</li><li><code>devSeqLengths::CuVector{Cint} = nothing</code>: Device copy of <code>seqLengthArray</code></li><li><code>fwdMode::cudnnForwardMode_t = CUDNN_FWD_MODE_INFERENCE</code>: set to <code>CUDNN_FWD_MODE_TRAINING</code> when training</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/rnn.jl#L1-L36">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnRNNForward!" href="#cuDNN.cudnnRNNForward!"><code>cuDNN.cudnnRNNForward!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnRNNForward(w, x; hiddenSize, o...)
cudnnRNNForward!(y, w, x; hiddenSize, o...)
cudnnRNNForward(w, x, d::cudnnRNNDescriptor; o...)
cudnnRNNForward!(y, w, x, d::cudnnRNNDescriptor; o...)</code></pre><p>Apply the RNN specified with weights <code>w</code> and configuration given by <code>d</code> or keyword options to input <code>x</code>.</p><p>Keyword arguments for hidden input/output:</p><ul><li><code>hx=nothing</code>: initialize the hidden vector if specified (by default initialized to 0).</li><li><code>cx=nothing</code>: initialize the cell vector (only in LSTMs) if specified (by default initialized to 0).</li><li><code>hy=nothing</code>: return the final hidden vector in <code>hy</code> if set to <code>Ref{Any}()</code>.</li><li><code>cy=nothing</code>: return the final cell vector in <code>cy</code> (only in LSTMs) if set to <code>Ref{Any}()</code>.</li></ul><p>Keyword arguments specifying the RNN when <code>d::cudnnRNNDescriptor</code> is not given:</p><ul><li><code>hiddenSize::Integer</code>: hidden vector size, which must be supplied when <code>d</code> is not given</li><li><code>algo::cudnnRNNAlgo_t = CUDNN_RNN_ALGO_STANDARD</code>: RNN algo (<code>CUDNN_RNN_ALGO_STANDARD</code>, <code>CUDNN_RNN_ALGO_PERSIST_STATIC</code>, or <code>CUDNN_RNN_ALGO_PERSIST_DYNAMIC</code>).</li><li><code>cellMode::cudnnRNNMode_t = CUDNN_LSTM</code>: Specifies the RNN cell type in the entire model (<code>CUDNN_RNN_RELU</code>, <code>CUDNN_RNN_TANH</code>, <code>CUDNN_LSTM</code>, <code>CUDNN_GRU</code>).</li><li><code>biasMode::cudnnRNNBiasMode_t = CUDNN_RNN_DOUBLE_BIAS</code>: Sets the number of bias vectors (<code>CUDNN_RNN_NO_BIAS</code>, <code>CUDNN_RNN_SINGLE_INP_BIAS</code>, <code>CUDNN_RNN_SINGLE_REC_BIAS</code>, <code>CUDNN_RNN_DOUBLE_BIAS</code>). The two single bias settings are functionally the same for <code>RELU</code>, <code>TANH</code> and <code>LSTM</code> cell types. For differences in <code>GRU</code> cells, see the description of <code>CUDNN_GRU</code> in cudnn docs.</li><li><code>dirMode::cudnnDirectionMode_t = CUDNN_UNIDIRECTIONAL</code>: Specifies the recurrence pattern: <code>CUDNN_UNIDIRECTIONAL</code> or <code>CUDNN_BIDIRECTIONAL</code>. In bidirectional RNNs, the hidden states passed between physical layers are concatenations of forward and backward hidden states.</li><li><code>inputMode::cudnnRNNInputMode_t = CUDNN_LINEAR_INPUT</code>: Specifies how the input to the RNN model is processed by the first layer. When <code>inputMode</code> is <code>CUDNN_LINEAR_INPUT</code>, original input vectors of size <code>inputSize</code> are multiplied by the weight matrix to obtain vectors of <code>hiddenSize</code>. When <code>inputMode</code> is <code>CUDNN_SKIP_INPUT</code>, the original input vectors to the first layer are used as is without multiplying them by the weight matrix.</li><li><code>mathPrec::DataType = eltype(x)</code>: This parameter is used to control the compute math precision in the RNN model. For <code>Float16</code> input/output can be <code>Float16</code> or <code>Float32</code>, for <code>Float32</code> or <code>Float64</code> input/output, must match the input/output type.</li><li><code>mathType::cudnnMathType_t = math_mode()</code>: Sets the preferred option to use NVIDIA Tensor Cores accelerators on Volta (SM 7.0) or higher GPUs. When <code>dataType</code> is <code>CUDNN_DATA_HALF</code>, the <code>mathType</code> parameter can be <code>CUDNN_DEFAULT_MATH</code> or <code>CUDNN_TENSOR_OP_MATH</code>. The <code>ALLOW_CONVERSION</code> setting is treated the same <code>CUDNN_TENSOR_OP_MATH</code> for this data type. When <code>dataType</code> is <code>CUDNN_DATA_FLOAT</code>, the <code>mathType</code> parameter can be <code>CUDNN_DEFAULT_MATH</code> or <code>CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION</code>. When the latter settings are used, original weights and intermediate results will be down-converted to <code>CUDNN_DATA_HALF</code> before they are used in another recursive iteration. When dataType is <code>CUDNN_DATA_DOUBLE</code>, the <code>mathType</code> parameter can be <code>CUDNN_DEFAULT_MATH</code>.</li><li><code>inputSize::Integer = size(x,1)</code>: Size of the input vector in the RNN model. When the <code>inputMode=CUDNN_SKIP_INPUT</code>, the <code>inputSize</code> should match the <code>hiddenSize</code> value.</li><li><code>projSize::Integer = hiddenSize</code>: The size of the LSTM cell output after the recurrent projection. This value should not be larger than <code>hiddenSize</code>. It is legal to set <code>projSize</code> equal to <code>hiddenSize</code>, however, in this case, the recurrent projection feature is disabled. The recurrent projection is an additional matrix multiplication in the LSTM cell to project hidden state vectors <code>ht</code> into smaller vectors <code>rt = Wr * ht</code>, where <code>Wr</code> is a rectangular matrix with <code>projSize</code> rows and hiddenSize columns. When the recurrent projection is enabled, the output of the LSTM cell (both to the next layer and unrolled in-time) is <code>rt</code> instead of <code>ht</code>. The recurrent projection can be enabled for LSTM cells and <code>CUDNN_RNN_ALGO_STANDARD</code> only.</li><li><code>numLayers::Integer = 1</code>: Number of stacked, physical layers in the deep RNN model. When <code>dirMode= CUDNN_BIDIRECTIONAL</code>, the physical layer consists of two pseudo-layers corresponding to forward and backward directions.</li><li><code>dropout::Real = 0</code>: When non-zero, dropout operation will be applied between physical layers. A single layer network will have no dropout applied. Dropout is used in the training mode only.</li><li><code>auxFlags::Integer = CUDNN_RNN_PADDED_IO_ENABLED</code>: Miscellaneous switches that do not require additional numerical values to configure the corresponding feature. In future cuDNN releases, this parameter will be used to extend the RNN functionality without adding new API functions (applicable options should be bitwise OR-ed). Currently, this parameter is used to enable or disable padded input/output (<code>CUDNN_RNN_PADDED_IO_DISABLED</code>, <code>CUDNN_RNN_PADDED_IO_ENABLED</code>). When the padded I/O is enabled, layouts <code>CUDNN_RNN_DATA_LAYOUT_SEQ_MAJOR_UNPACKED</code> and <code>CUDNN_RNN_DATA_LAYOUT_BATCH_MAJOR_UNPACKED</code> are permitted in RNN data descriptors.</li></ul><p>Other keyword arguments:</p><ul><li><code>layout::cudnnRNNDataLayout_t = CUDNN_RNN_DATA_LAYOUT_SEQ_MAJOR_UNPACKED</code>: The memory layout of the RNN data tensor. Options are <code>CUDNN_RNN_DATA_LAYOUT_SEQ_MAJOR_UNPACKED</code>: Data layout is padded, with outer stride from one time-step to the next; <code>CUDNN_RNN_DATA_LAYOUT_SEQ_MAJOR_PACKED</code>: The sequence length is sorted and packed as in the basic RNN API; <code>CUDNN_RNN_DATA_LAYOUT_BATCH_MAJOR_UNPACKED</code>: Data layout is padded, with outer stride from one batch to the next.</li><li><code>seqLengthArray::Vector{Cint} = nothing</code>: An integer array with <code>batchSize</code> number of elements. Describes the length (number of time-steps) of each sequence. Each element in <code>seqLengthArray</code> must be greater than or equal to 0 but less than or equal to <code>maxSeqLength</code>. In the packed layout, the elements should be sorted in descending order, similar to the layout required by the non-extended RNN compute functions. The default value <code>nothing</code> assumes uniform <code>seqLength</code>s, no padding.</li><li><code>devSeqLengths::CuVector{Cint} = nothing</code>: Device copy of <code>seqLengthArray</code></li><li><code>fwdMode::cudnnForwardMode_t = CUDNN_FWD_MODE_INFERENCE</code>: set to <code>CUDNN_FWD_MODE_TRAINING</code> when training</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/rnn.jl#L1-L36">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnReduceTensor" href="#cuDNN.cudnnReduceTensor"><code>cuDNN.cudnnReduceTensor</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnReduceTensor(x; dims, op, compType, nanOpt, indices, alpha)
cudnnReduceTensor(x, d::cudnnReduceTensorDescriptor; dims, indices, alpha)
cudnnReduceTensor!(y, x; op, compType, nanOpt, indices, alpha, beta)
cudnnReduceTensor!(y, x, d::cudnnReduceTensorDescriptor; indices, alpha, beta)</code></pre><p>Return the result of the specified reduction operation applied to <code>x</code>.  Optionally <code>y</code> holds the result and <code>d</code> specifies the operation.  Each dimension of the output tensor <code>y</code> must match the corresponding dimension of the input tensor <code>x</code> or must be equal to 1. The dimensions equal to 1 indicate the dimensions of <code>x</code> to be reduced.  Keyword arguments:</p><ul><li><code>dims = ntuple(i-&gt;1,ndims(x))</code>: specifies the shape of the output when <code>y</code> is not given</li><li><code>indices = nothing</code>: previously allocated space for writing indices which can be generated for min and max ops only, can be a <code>CuArray</code> of <code>UInt8</code>, <code>UInt16</code>, <code>UInt32</code> or <code>UInt64</code></li><li><code>alpha=1, beta=0</code> are used for scaling, i.e. <code>y .= alpha * op.(x1) .+ beta * y</code></li></ul><p>Keyword arguments that can be used when <code>reduceTensorDesc</code> is not specified:</p><ul><li><code>op = CUDNN_REDUCE_TENSOR_ADD</code>: Reduction operation, <code>ADD</code> can be replaced with <code>MUL</code>, <code>MIN</code>, <code>MAX</code>, <code>AMAX</code>, <code>AVG</code>, <code>NORM1</code>, <code>NORM2</code>, <code>MUL_NO_ZEROS</code></li><li><code>compType = (eltype(x) &lt;: Float64 ? Float64 : Float32)</code>: Computation datatype</li><li><code>nanOpt = CUDNN_NOT_PROPAGATE_NAN</code>: NaN propagation policy, the other option is <code>CUDNN_PROPAGATE_NAN</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/reduce.jl#L3-L22">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnReduceTensor!" href="#cuDNN.cudnnReduceTensor!"><code>cuDNN.cudnnReduceTensor!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnReduceTensor(x; dims, op, compType, nanOpt, indices, alpha)
cudnnReduceTensor(x, d::cudnnReduceTensorDescriptor; dims, indices, alpha)
cudnnReduceTensor!(y, x; op, compType, nanOpt, indices, alpha, beta)
cudnnReduceTensor!(y, x, d::cudnnReduceTensorDescriptor; indices, alpha, beta)</code></pre><p>Return the result of the specified reduction operation applied to <code>x</code>.  Optionally <code>y</code> holds the result and <code>d</code> specifies the operation.  Each dimension of the output tensor <code>y</code> must match the corresponding dimension of the input tensor <code>x</code> or must be equal to 1. The dimensions equal to 1 indicate the dimensions of <code>x</code> to be reduced.  Keyword arguments:</p><ul><li><code>dims = ntuple(i-&gt;1,ndims(x))</code>: specifies the shape of the output when <code>y</code> is not given</li><li><code>indices = nothing</code>: previously allocated space for writing indices which can be generated for min and max ops only, can be a <code>CuArray</code> of <code>UInt8</code>, <code>UInt16</code>, <code>UInt32</code> or <code>UInt64</code></li><li><code>alpha=1, beta=0</code> are used for scaling, i.e. <code>y .= alpha * op.(x1) .+ beta * y</code></li></ul><p>Keyword arguments that can be used when <code>reduceTensorDesc</code> is not specified:</p><ul><li><code>op = CUDNN_REDUCE_TENSOR_ADD</code>: Reduction operation, <code>ADD</code> can be replaced with <code>MUL</code>, <code>MIN</code>, <code>MAX</code>, <code>AMAX</code>, <code>AVG</code>, <code>NORM1</code>, <code>NORM2</code>, <code>MUL_NO_ZEROS</code></li><li><code>compType = (eltype(x) &lt;: Float64 ? Float64 : Float32)</code>: Computation datatype</li><li><code>nanOpt = CUDNN_NOT_PROPAGATE_NAN</code>: NaN propagation policy, the other option is <code>CUDNN_PROPAGATE_NAN</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/reduce.jl#L3-L22">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnScaleTensor" href="#cuDNN.cudnnScaleTensor"><code>cuDNN.cudnnScaleTensor</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnScaleTensor(x, s)
cudnnScaleTensor!(y, x, s)</code></pre><p>Scale all elements of tensor <code>x</code> with scale <code>s</code> and return the result. <code>cudnnScaleTensor</code> allocates a new array for the answer, <code>cudnnScaleTensor!</code> overwrites <code>y</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/inplace.jl#L16-L22">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnScaleTensor!" href="#cuDNN.cudnnScaleTensor!"><code>cuDNN.cudnnScaleTensor!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnScaleTensor(x, s)
cudnnScaleTensor!(y, x, s)</code></pre><p>Scale all elements of tensor <code>x</code> with scale <code>s</code> and return the result. <code>cudnnScaleTensor</code> allocates a new array for the answer, <code>cudnnScaleTensor!</code> overwrites <code>y</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/inplace.jl#L16-L22">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnSetTensor!-Tuple{Any, Real}" href="#cuDNN.cudnnSetTensor!-Tuple{Any, Real}"><code>cuDNN.cudnnSetTensor!</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnSetTensor!(x, s)</code></pre><p>Set all elements of tensor <code>x</code> to scalar <code>s</code> and return <code>x</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/inplace.jl#L1-L5">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnSoftmaxForward" href="#cuDNN.cudnnSoftmaxForward"><code>cuDNN.cudnnSoftmaxForward</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnSoftmaxForward(x; algo, mode, alpha)
cudnnSoftmaxForward!(y, x; algo, mode, alpha, beta)</code></pre><p>Return the softmax or logsoftmax of the input <code>x</code> depending on the <code>algo</code> keyword argument. The <code>y</code> argument holds the result and it should be similar to <code>x</code> if specified. Keyword arguments:</p><ul><li><code>algo = (CUDA.math_mode()===CUDA.FAST_MATH ? CUDNN_SOFTMAX_FAST : CUDNN_SOFTMAX_ACCURATE)</code>: Options are <code>CUDNN_SOFTMAX_ACCURATE</code> which subtracts max from every point to avoid overflow, <code>CUDNN_SOFTMAX_FAST</code> which doesn&#39;t and <code>CUDNN_SOFTMAX_LOG</code> which returns logsoftmax.</li><li><code>mode = CUDNN_SOFTMAX_MODE_INSTANCE</code>: Compute softmax per image (N) across the dimensions C,H,W. <code>CUDNN_SOFTMAX_MODE_CHANNEL</code> computes softmax per spatial location (H,W) per image (N) across the dimension C. </li><li><code>alpha=1, beta=0</code> can be used for scaling, i.e. <code>y .= alpha * op(x1) .+ beta * y</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/softmax.jl#L1-L12">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.cudnnSoftmaxForward!" href="#cuDNN.cudnnSoftmaxForward!"><code>cuDNN.cudnnSoftmaxForward!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cudnnSoftmaxForward(x; algo, mode, alpha)
cudnnSoftmaxForward!(y, x; algo, mode, alpha, beta)</code></pre><p>Return the softmax or logsoftmax of the input <code>x</code> depending on the <code>algo</code> keyword argument. The <code>y</code> argument holds the result and it should be similar to <code>x</code> if specified. Keyword arguments:</p><ul><li><code>algo = (CUDA.math_mode()===CUDA.FAST_MATH ? CUDNN_SOFTMAX_FAST : CUDNN_SOFTMAX_ACCURATE)</code>: Options are <code>CUDNN_SOFTMAX_ACCURATE</code> which subtracts max from every point to avoid overflow, <code>CUDNN_SOFTMAX_FAST</code> which doesn&#39;t and <code>CUDNN_SOFTMAX_LOG</code> which returns logsoftmax.</li><li><code>mode = CUDNN_SOFTMAX_MODE_INSTANCE</code>: Compute softmax per image (N) across the dimensions C,H,W. <code>CUDNN_SOFTMAX_MODE_CHANNEL</code> computes softmax per spatial location (H,W) per image (N) across the dimension C. </li><li><code>alpha=1, beta=0</code> can be used for scaling, i.e. <code>y .= alpha * op(x1) .+ beta * y</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/softmax.jl#L1-L12">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.sdim-Tuple{Any, Any, Any}" href="#cuDNN.sdim-Tuple{Any, Any, Any}"><code>cuDNN.sdim</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">sdim(x,axes,dim)
sdim(x,axes)</code></pre><p>The first form returns the size of <code>x</code> in the dimension specified with <code>dim::cudnnSeqDataAxis_t</code> (e.g. CUDNN<em>SEQDATA</em>TIME_DIM), i.e. return <code>size(x,i)</code> such that <code>axes[i]==dim</code>.</p><p>The second form returns an array of length 4 <code>dims::Vector{Cint}</code> such that <code>dims[1+dim] == sdim(x,axes,dim)</code> where <code>dim::cudnnSeqDataAxis_t</code> specifies the role of the dimension (e.g. dims[CUDNN<em>SEQDATA</em>TIME_DIM]==5).</p><p>The <code>axes::Vector{cudnnSeqDataAxis_t}</code> argument is an array of length 4 that specifies the role of Julia dimensions, e.g. <code>axes[3]=CUDNN_SEQDATA_TIME_DIM</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/multiheadattn.jl#L238-L252">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="cuDNN.@cudnnDescriptor" href="#cuDNN.@cudnnDescriptor"><code>cuDNN.@cudnnDescriptor</code></a> — <span class="docstring-category">Macro</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">@cudnnDescriptor(XXX, setter=cudnnSetXXXDescriptor)</code></pre><p>Defines a new type <code>cudnnXXXDescriptor</code> with a single field <code>ptr::cudnnXXXDescriptor_t</code> and its constructor. The second optional argument is the function that sets the descriptor fields and defaults to <code>cudnnSetXXXDescriptor</code>. The constructor is memoized, i.e. when called with the same arguments it returns the same object rather than creating a new one.</p><p>The arguments of the constructor and thus the keys to the memoization cache depend on the setter: If the setter has arguments <code>cudnnSetXXXDescriptor(ptr::cudnnXXXDescriptor_t, args...)</code>, then the constructor has <code>cudnnXXXDescriptor(args...)</code>. The user can control these arguments by defining a custom setter.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/3d41537bb4db7516fba9d2d68c0761c109329b9a/lib/cudnn/src/descriptors.jl#L4-L16">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../cudadrv/">« CUDA driver</a><a class="docs-footer-nextpage" href="../cufft/">CUFFT »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.15.0 on <span class="colophon-date" title="Thursday 29 January 2026 13:59">Thursday 29 January 2026</span>. Using Julia version 1.11.8.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
