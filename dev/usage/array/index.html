<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Array programming · CUDA.jl</title><meta name="title" content="Array programming · CUDA.jl"/><meta property="og:title" content="Array programming · CUDA.jl"/><meta property="twitter:title" content="Array programming · CUDA.jl"/><meta name="description" content="Documentation for CUDA.jl."/><meta property="og:description" content="Documentation for CUDA.jl."/><meta property="twitter:description" content="Documentation for CUDA.jl."/><meta property="og:url" content="https://cuda.juliagpu.org/stable/usage/array/"/><meta property="twitter:url" content="https://cuda.juliagpu.org/stable/usage/array/"/><link rel="canonical" href="https://cuda.juliagpu.org/stable/usage/array/"/><script async src="https://www.googletagmanager.com/gtag/js?id=UA-154489943-2"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-154489943-2', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="CUDA.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">CUDA.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/introduction/">Introduction</a></li><li><a class="tocitem" href="../../tutorials/custom_structs/">Using custom structs</a></li><li><a class="tocitem" href="../../tutorials/performance/">Performance Tips</a></li></ul></li><li><span class="tocitem">Installation</span><ul><li><a class="tocitem" href="../../installation/overview/">Overview</a></li><li><a class="tocitem" href="../../installation/conditional/">Conditional use</a></li><li><a class="tocitem" href="../../installation/troubleshooting/">Troubleshooting</a></li></ul></li><li><span class="tocitem">Usage</span><ul><li><a class="tocitem" href="../overview/">Overview</a></li><li><a class="tocitem" href="../workflow/">Workflow</a></li><li class="is-active"><a class="tocitem" href>Array programming</a><ul class="internal"><li><a class="tocitem" href="#Construction-and-Initialization"><span>Construction and Initialization</span></a></li><li><a class="tocitem" href="#Higher-order-abstractions"><span>Higher-order abstractions</span></a></li><li><a class="tocitem" href="#Logical-operations"><span>Logical operations</span></a></li><li><a class="tocitem" href="#Array-wrappers"><span>Array wrappers</span></a></li><li><a class="tocitem" href="#Random-numbers"><span>Random numbers</span></a></li><li><a class="tocitem" href="#Linear-algebra"><span>Linear algebra</span></a></li><li><a class="tocitem" href="#Solver"><span>Solver</span></a></li><li><a class="tocitem" href="#Sparse-arrays"><span>Sparse arrays</span></a></li><li><a class="tocitem" href="#FFTs"><span>FFTs</span></a></li></ul></li><li><a class="tocitem" href="../memory/">Memory management</a></li><li><a class="tocitem" href="../multitasking/">Tasks and threads</a></li><li><a class="tocitem" href="../multigpu/">Multiple GPUs</a></li></ul></li><li><span class="tocitem">Development</span><ul><li><a class="tocitem" href="../../development/profiling/">Benchmarking &amp; profiling</a></li><li><a class="tocitem" href="../../development/kernel/">Kernel programming</a></li><li><a class="tocitem" href="../../development/troubleshooting/">Troubleshooting</a></li><li><a class="tocitem" href="../../development/debugging/">Debugging</a></li></ul></li><li><span class="tocitem">API reference</span><ul><li><a class="tocitem" href="../../api/essentials/">Essentials</a></li><li><a class="tocitem" href="../../api/array/">Array programming</a></li><li><a class="tocitem" href="../../api/kernel/">Kernel programming</a></li><li><a class="tocitem" href="../../api/compiler/">Compiler</a></li></ul></li><li><span class="tocitem">Library reference</span><ul><li><a class="tocitem" href="../../lib/driver/">CUDA driver</a></li></ul></li><li><a class="tocitem" href="../../faq/">FAQ</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Usage</a></li><li class="is-active"><a href>Array programming</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Array programming</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaGPU/CUDA.jl/blob/master/docs/src/usage/array.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Array-programming"><a class="docs-heading-anchor" href="#Array-programming">Array programming</a><a id="Array-programming-1"></a><a class="docs-heading-anchor-permalink" href="#Array-programming" title="Permalink"></a></h1><p>The easiest way to use the GPU&#39;s massive parallelism, is by expressing operations in terms of arrays: CUDA.jl provides an array type, <code>CuArray</code>, and many specialized array operations that execute efficiently on the GPU hardware. In this section, we will briefly demonstrate use of the <code>CuArray</code> type. Since we expose CUDA&#39;s functionality by implementing existing Julia interfaces on the <code>CuArray</code> type, you should refer to the <a href="https://docs.julialang.org">upstream Julia documentation</a> for more information on these operations.</p><p>If you encounter missing functionality, or are running into operations that trigger so-called <a href="../workflow/#UsageWorkflowScalar">&quot;scalar iteration&quot;</a>, have a look at the <a href="https://github.com/JuliaGPU/CUDA.jl/issues">issue tracker</a> and file a new issue if there&#39;s none. Do note that you can always access the underlying CUDA APIs by calling into the relevant submodule. For example, if parts of the Random interface isn&#39;t properly implemented by CUDA.jl, you can look at the CURAND documentation and possibly call methods from the <code>CURAND</code> submodule directly. These submodules are available after importing the CUDA package.</p><h2 id="Construction-and-Initialization"><a class="docs-heading-anchor" href="#Construction-and-Initialization">Construction and Initialization</a><a id="Construction-and-Initialization-1"></a><a class="docs-heading-anchor-permalink" href="#Construction-and-Initialization" title="Permalink"></a></h2><p>The <code>CuArray</code> type aims to implement the <code>AbstractArray</code> interface, and provide implementations of methods that are commonly used when working with arrays. That means you can construct <code>CuArray</code>s in the same way as regular <code>Array</code> objects:</p><pre><code class="language-julia hljs">julia&gt; CuArray{Int}(undef, 2)
2-element CuArray{Int64, 1}:
 0
 0

julia&gt; CuArray{Int}(undef, (1,2))
1×2 CuArray{Int64, 2}:
 0  0

julia&gt; similar(ans)
1×2 CuArray{Int64, 2}:
 0  0</code></pre><p>Copying memory to or from the GPU can be expressed using constructors as well, or by calling <code>copyto!</code>:</p><pre><code class="language-julia-repl hljs">julia&gt; a = CuArray([1,2])
2-element CuArray{Int64, 1, CUDA.Mem.DeviceBuffer}:
 1
 2

julia&gt; b = Array(a)
2-element Vector{Int64}:
 1
 2

julia&gt; copyto!(b, a)
2-element Vector{Int64}:
 1
 2</code></pre><h2 id="Higher-order-abstractions"><a class="docs-heading-anchor" href="#Higher-order-abstractions">Higher-order abstractions</a><a id="Higher-order-abstractions-1"></a><a class="docs-heading-anchor-permalink" href="#Higher-order-abstractions" title="Permalink"></a></h2><p>The real power of programming GPUs with arrays comes from Julia&#39;s higher-order array abstractions: Operations that take user code as an argument, and specialize execution on it. With these functions, you can often avoid having to write custom kernels. For example, to perform simple element-wise operations you can use <code>map</code> or <code>broadcast</code>:</p><pre><code class="language-julia-repl hljs">julia&gt; a = CuArray{Float32}(undef, (1,2));

julia&gt; a .= 5
1×2 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:
 5.0  5.0

julia&gt; map(sin, a)
1×2 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:
 -0.958924  -0.958924</code></pre><p>To reduce the dimensionality of arrays, CUDA.jl implements the various flavours of <code>(map)reduce(dim)</code>:</p><pre><code class="language-julia-repl hljs">julia&gt; a = CUDA.ones(2,3)
2×3 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:
 1.0  1.0  1.0
 1.0  1.0  1.0

julia&gt; reduce(+, a)
6.0f0

julia&gt; mapreduce(sin, *, a; dims=2)
2×1 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:
 0.59582335
 0.59582335

julia&gt; b = CUDA.zeros(1)
1-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:
 0.0

julia&gt; Base.mapreducedim!(identity, +, b, a)
1×1 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:
 6.0</code></pre><p>To retain intermediate values, you can use <code>accumulate</code>:</p><pre><code class="language-julia-repl hljs">julia&gt; a = CUDA.ones(2,3)
2×3 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:
 1.0  1.0  1.0
 1.0  1.0  1.0

julia&gt; accumulate(+, a; dims=2)
2×3 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:
 1.0  2.0  3.0
 1.0  2.0  3.0</code></pre><p>Be wary that the operator <code>f</code> of <code>accumulate</code>, <code>accumulate!</code>, <code>scan</code> and <code>scan!</code> must be associative since the operation is performed in parallel. That is <code>f(f(a,b)c)</code> must be equivalent to <code>f(a,f(b,c))</code>. Accumulating with a non-associative operator on a <code>CuArray</code> will not produce the same result as on an <code>Array</code>.</p><h2 id="Logical-operations"><a class="docs-heading-anchor" href="#Logical-operations">Logical operations</a><a id="Logical-operations-1"></a><a class="docs-heading-anchor-permalink" href="#Logical-operations" title="Permalink"></a></h2><p><code>CuArray</code>s can also be indexed with arrays of boolean values to select items:</p><pre><code class="language-julia-repl hljs">julia&gt; a = CuArray([1,2,3])
3-element CuArray{Int64, 1, CUDA.Mem.DeviceBuffer}:
 1
 2
 3

julia&gt; a[[false,true,false]]
1-element CuArray{Int64, 1, CUDA.Mem.DeviceBuffer}:
 2</code></pre><p>Built on top of this, are several functions with higher-level semantics:</p><pre><code class="language-julia-repl hljs">julia&gt; a = CuArray([11,12,13])
3-element CuArray{Int64, 1, CUDA.Mem.DeviceBuffer}:
 11
 12
 13

julia&gt; findall(isodd, a)
2-element CuArray{Int64, 1, CUDA.Mem.DeviceBuffer}:
 1
 3

julia&gt; findfirst(isodd, a)
1

julia&gt; b = CuArray([11 12 13; 21 22 23])
2×3 CuArray{Int64, 2, CUDA.Mem.DeviceBuffer}:
 11  12  13
 21  22  23

julia&gt; findmin(b)
(11, CartesianIndex(1, 1))

julia&gt; findmax(b; dims=2)
([13; 23;;], CartesianIndex{2}[CartesianIndex(1, 3); CartesianIndex(2, 3);;])</code></pre><h2 id="Array-wrappers"><a class="docs-heading-anchor" href="#Array-wrappers">Array wrappers</a><a id="Array-wrappers-1"></a><a class="docs-heading-anchor-permalink" href="#Array-wrappers" title="Permalink"></a></h2><p>To some extent, CUDA.jl also supports well-known array wrappers from the standard library:</p><pre><code class="language-julia-repl hljs">julia&gt; a = CuArray(collect(1:10))
10-element CuArray{Int64, 1, CUDA.Mem.DeviceBuffer}:
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10

julia&gt; a = CuArray(collect(1:6))
6-element CuArray{Int64, 1, CUDA.Mem.DeviceBuffer}:
 1
 2
 3
 4
 5
 6

julia&gt; b = reshape(a, (2,3))
2×3 CuArray{Int64, 2, CUDA.Mem.DeviceBuffer}:
 1  3  5
 2  4  6

julia&gt; c = view(a, 2:5)
4-element CuArray{Int64, 1, CUDA.Mem.DeviceBuffer}:
 2
 3
 4
 5</code></pre><p>The above contiguous <code>view</code> and <code>reshape</code> have been specialized to return new objects of type <code>CuArray</code>. Other wrappers, such as non-contiguous views or the LinearAlgebra wrappers that will be discussed below, are implemented using their own type (e.g. <code>SubArray</code> or <code>Transpose</code>). This can cause problems, as calling methods with these wrapped objects will not dispatch to specialized <code>CuArray</code> methods anymore. That may result in a call to fallback functionality that performs scalar iteration.</p><p>Certain common operations, like broadcast or matrix multiplication, do know how to deal with array wrappers by using the <a href="https://github.com/JuliaGPU/Adapt.jl">Adapt.jl</a> package. This is still not a complete solution though, e.g. new array wrappers are not covered, and only one level of wrapping is supported. Sometimes the only solution is to materialize the wrapper to a <code>CuArray</code> again.</p><h2 id="Random-numbers"><a class="docs-heading-anchor" href="#Random-numbers">Random numbers</a><a id="Random-numbers-1"></a><a class="docs-heading-anchor-permalink" href="#Random-numbers" title="Permalink"></a></h2><p>Base&#39;s convenience functions for generating random numbers are available in the CUDA module as well:</p><pre><code class="language-julia-repl hljs">julia&gt; CUDA.rand(2)
2-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:
 0.74021935
 0.9209938

julia&gt; CUDA.randn(Float64, 2, 1)
2×1 CuArray{Float64, 2, CUDA.Mem.DeviceBuffer}:
 -0.3893830994647195
  1.618410515635752</code></pre><p>Behind the scenes, these random numbers come from two different generators: one backed by <a href="https://docs.nvidia.com/cuda/curand/index.html">CURAND</a>, another by kernels defined in CUDA.jl. Operations on these generators are implemented using methods from the Random standard library:</p><pre><code class="language-julia-repl hljs">julia&gt; using Random

julia&gt; a = Random.rand(CURAND.default_rng(), Float32, 1)
1-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:
 0.74021935

julia&gt; a = Random.rand!(CUDA.default_rng(), a)
1-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:
 0.46691537</code></pre><p>CURAND also supports generating lognormal and Poisson-distributed numbers:</p><pre><code class="language-julia-repl hljs">julia&gt; CUDA.rand_logn(Float32, 1, 5; mean=2, stddev=20)
1×5 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:
 2567.61  4.256f-6  54.5948  0.00283999  9.81175f22

julia&gt; CUDA.rand_poisson(UInt32, 1, 10; lambda=100)
1×10 CuArray{UInt32, 2, CUDA.Mem.DeviceBuffer}:
 0x00000058  0x00000066  0x00000061  …  0x0000006b  0x0000005f  0x00000069</code></pre><p>Note that these custom operations are only supported on a subset of types.</p><h2 id="Linear-algebra"><a class="docs-heading-anchor" href="#Linear-algebra">Linear algebra</a><a id="Linear-algebra-1"></a><a class="docs-heading-anchor-permalink" href="#Linear-algebra" title="Permalink"></a></h2><p>CUDA&#39;s linear algebra functionality from the <a href="https://developer.nvidia.com/cublas">CUBLAS</a> library is exposed by implementing methods in the LinearAlgebra standard library:</p><pre><code class="language-julia hljs">julia&gt; # enable logging to demonstrate a CUBLAS kernel is used
       CUBLAS.cublasLoggerConfigure(1, 0, 1, C_NULL)

julia&gt; CUDA.rand(2,2) * CUDA.rand(2,2)
I! cuBLAS (v10.2) function cublasStatus_t cublasSgemm_v2(cublasContext*, cublasOperation_t, cublasOperation_t, int, int, int, const float*, const float*, int, const float*, int, const float*, float*, int) called
2×2 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:
 0.295727  0.479395
 0.624576  0.557361</code></pre><p>Certain operations, like the above matrix-matrix multiplication, also have a native fallback written in Julia for the purpose of working with types that are not supported by CUBLAS:</p><pre><code class="language-julia hljs">julia&gt; # enable logging to demonstrate no CUBLAS kernel is used
       CUBLAS.cublasLoggerConfigure(1, 0, 1, C_NULL)

julia&gt; CUDA.rand(Int128, 2, 2) * CUDA.rand(Int128, 2, 2)
2×2 CuArray{Int128, 2, CUDA.Mem.DeviceBuffer}:
 -147256259324085278916026657445395486093  -62954140705285875940311066889684981211
 -154405209690443624360811355271386638733  -77891631198498491666867579047988353207</code></pre><p>Operations that exist in CUBLAS, but are not (yet) covered by high-level constructs in the LinearAlgebra standard library, can be accessed directly from the CUBLAS submodule. Note that you do not need to call the C wrappers directly (e.g. <code>cublasDdot</code>), as many operations have more high-level wrappers available as well (e.g. <code>dot</code>):</p><pre><code class="language-julia-repl hljs">julia&gt; x = CUDA.rand(2)
2-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:
 0.74021935
 0.9209938

julia&gt; y = CUDA.rand(2)
2-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:
 0.03902049
 0.9689629

julia&gt; CUBLAS.dot(2, x, y)
0.92129254f0

julia&gt; using LinearAlgebra

julia&gt; dot(Array(x), Array(y))
0.92129254f0</code></pre><h2 id="Solver"><a class="docs-heading-anchor" href="#Solver">Solver</a><a id="Solver-1"></a><a class="docs-heading-anchor-permalink" href="#Solver" title="Permalink"></a></h2><p>LAPACK-like functionality as found in the <a href="https://docs.nvidia.com/cuda/cusolver/index.html">CUSOLVER</a> library can be accessed through methods in the LinearAlgebra standard library too:</p><pre><code class="language-julia-repl hljs">julia&gt; using LinearAlgebra

julia&gt; a = CUDA.rand(2,2)
2×2 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:
 0.740219  0.0390205
 0.920994  0.968963

julia&gt; a = a * a&#39;
2×2 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:
 0.549447  0.719547
 0.719547  1.78712

julia&gt; cholesky(a)
Cholesky{Float32, CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}}
U factor:
2×2 UpperTriangular{Float32, CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}}:
 0.741247  0.970725
  ⋅        0.919137</code></pre><p>Other operations are bound to the left-division operator:</p><pre><code class="language-julia-repl hljs">julia&gt; a = CUDA.rand(2,2)
2×2 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:
 0.740219  0.0390205
 0.920994  0.968963

julia&gt; b = CUDA.rand(2,2)
2×2 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:
 0.925141  0.667319
 0.44635   0.109931

julia&gt; a \ b
2×2 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:
  1.29018    0.942773
 -0.765663  -0.782648

julia&gt; Array(a) \ Array(b)
2×2 Matrix{Float32}:
  1.29018    0.942773
 -0.765663  -0.782648</code></pre><h2 id="Sparse-arrays"><a class="docs-heading-anchor" href="#Sparse-arrays">Sparse arrays</a><a id="Sparse-arrays-1"></a><a class="docs-heading-anchor-permalink" href="#Sparse-arrays" title="Permalink"></a></h2><p>Sparse array functionality from the <a href="https://docs.nvidia.com/cuda/cusparse/index.html">CUSPARSE</a> library is mainly available through functionality from the SparseArrays package applied to <code>CuSparseArray</code> objects:</p><pre><code class="language-julia-repl hljs">julia&gt; using SparseArrays

julia&gt; x = sprand(10,0.2)
10-element SparseVector{Float64, Int64} with 5 stored entries:
  [2 ]  =  0.538639
  [4 ]  =  0.89699
  [6 ]  =  0.258478
  [7 ]  =  0.338949
  [10]  =  0.424742

julia&gt; using CUDA.CUSPARSE

julia&gt; d_x = CuSparseVector(x)
10-element CuSparseVector{Float64, Int32} with 5 stored entries:
  [2 ]  =  0.538639
  [4 ]  =  0.89699
  [6 ]  =  0.258478
  [7 ]  =  0.338949
  [10]  =  0.424742

julia&gt; nonzeros(d_x)
5-element CuArray{Float64, 1, CUDA.Mem.DeviceBuffer}:
 0.538639413965653
 0.8969897902567084
 0.25847781536337067
 0.3389490517221738
 0.4247416640213063

julia&gt; nnz(d_x)
5</code></pre><p>For 2-D arrays the <code>CuSparseMatrixCSC</code> and <code>CuSparseMatrixCSR</code> can be used.</p><p>Non-integrated functionality can be access directly in the CUSPARSE submodule again.</p><h2 id="FFTs"><a class="docs-heading-anchor" href="#FFTs">FFTs</a><a id="FFTs-1"></a><a class="docs-heading-anchor-permalink" href="#FFTs" title="Permalink"></a></h2><p>Functionality from <a href="https://docs.nvidia.com/cuda/cufft/index.html">CUFFT</a> is integrated with the interfaces from the <a href="https://github.com/JuliaMath/AbstractFFTs.jl">AbstractFFTs.jl</a> package:</p><pre><code class="language-julia-repl hljs">julia&gt; a = CUDA.rand(2,2)
2×2 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:
 0.740219  0.0390205
 0.920994  0.968963

julia&gt; using CUDA.CUFFT

julia&gt; fft(a)
2×2 CuArray{ComplexF32, 2, CUDA.Mem.DeviceBuffer}:
   2.6692+0.0im   0.65323+0.0im
 -1.11072+0.0im  0.749168+0.0im</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../workflow/">« Workflow</a><a class="docs-footer-nextpage" href="../memory/">Memory management »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.1.2 on <span class="colophon-date" title="Wednesday 17 January 2024 08:15">Wednesday 17 January 2024</span>. Using Julia version 1.9.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
