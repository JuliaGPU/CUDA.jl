<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Kernel programming · CUDA.jl</title><meta name="title" content="Kernel programming · CUDA.jl"/><meta property="og:title" content="Kernel programming · CUDA.jl"/><meta property="twitter:title" content="Kernel programming · CUDA.jl"/><meta name="description" content="Documentation for CUDA.jl."/><meta property="og:description" content="Documentation for CUDA.jl."/><meta property="twitter:description" content="Documentation for CUDA.jl."/><meta property="og:url" content="https://cuda.juliagpu.org/stable/api/kernel/"/><meta property="twitter:url" content="https://cuda.juliagpu.org/stable/api/kernel/"/><link rel="canonical" href="https://cuda.juliagpu.org/stable/api/kernel/"/><script async src="https://www.googletagmanager.com/gtag/js?id=UA-154489943-2"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-154489943-2', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="CUDA.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">CUDA.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/introduction/">Introduction</a></li><li><a class="tocitem" href="../../tutorials/custom_structs/">Using custom structs</a></li><li><a class="tocitem" href="../../tutorials/performance/">Performance Tips</a></li></ul></li><li><span class="tocitem">Installation</span><ul><li><a class="tocitem" href="../../installation/overview/">Overview</a></li><li><a class="tocitem" href="../../installation/conditional/">Conditional use</a></li><li><a class="tocitem" href="../../installation/troubleshooting/">Troubleshooting</a></li></ul></li><li><span class="tocitem">Usage</span><ul><li><a class="tocitem" href="../../usage/overview/">Overview</a></li><li><a class="tocitem" href="../../usage/workflow/">Workflow</a></li><li><a class="tocitem" href="../../usage/array/">Array programming</a></li><li><a class="tocitem" href="../../usage/memory/">Memory management</a></li><li><a class="tocitem" href="../../usage/multitasking/">Tasks and threads</a></li><li><a class="tocitem" href="../../usage/multigpu/">Multiple GPUs</a></li></ul></li><li><span class="tocitem">Development</span><ul><li><a class="tocitem" href="../../development/profiling/">Benchmarking &amp; profiling</a></li><li><a class="tocitem" href="../../development/kernel/">Kernel programming</a></li><li><a class="tocitem" href="../../development/troubleshooting/">Troubleshooting</a></li><li><a class="tocitem" href="../../development/debugging/">Debugging</a></li></ul></li><li><span class="tocitem">API reference</span><ul><li><a class="tocitem" href="../essentials/">Essentials</a></li><li><a class="tocitem" href="../array/">Array programming</a></li><li class="is-active"><a class="tocitem" href>Kernel programming</a><ul class="internal"><li><a class="tocitem" href="#Indexing-and-dimensions"><span>Indexing and dimensions</span></a></li><li><a class="tocitem" href="#Device-arrays"><span>Device arrays</span></a></li><li><a class="tocitem" href="#Memory-types"><span>Memory types</span></a></li><li><a class="tocitem" href="#Synchronization"><span>Synchronization</span></a></li><li><a class="tocitem" href="#Time-functions"><span>Time functions</span></a></li><li><a class="tocitem" href="#Warp-level-functions"><span>Warp-level functions</span></a></li><li><a class="tocitem" href="#Formatted-Output"><span>Formatted Output</span></a></li><li><a class="tocitem" href="#Assertions"><span>Assertions</span></a></li><li><a class="tocitem" href="#Atomics"><span>Atomics</span></a></li><li><a class="tocitem" href="#Dynamic-parallelism"><span>Dynamic parallelism</span></a></li><li><a class="tocitem" href="#Cooperative-groups"><span>Cooperative groups</span></a></li><li><a class="tocitem" href="#Data-transfer"><span>Data transfer</span></a></li><li><a class="tocitem" href="#Math"><span>Math</span></a></li><li><a class="tocitem" href="#WMMA"><span>WMMA</span></a></li><li><a class="tocitem" href="#Other"><span>Other</span></a></li></ul></li><li><a class="tocitem" href="../compiler/">Compiler</a></li></ul></li><li><span class="tocitem">Library reference</span><ul><li><a class="tocitem" href="../../lib/driver/">CUDA driver</a></li></ul></li><li><a class="tocitem" href="../../faq/">FAQ</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API reference</a></li><li class="is-active"><a href>Kernel programming</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Kernel programming</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaGPU/CUDA.jl/blob/master/docs/src/api/kernel.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="KernelAPI"><a class="docs-heading-anchor" href="#KernelAPI">Kernel programming</a><a id="KernelAPI-1"></a><a class="docs-heading-anchor-permalink" href="#KernelAPI" title="Permalink"></a></h1><p>This section lists the package&#39;s public functionality that corresponds to special CUDA functions for use in device code. It is loosely organized according to the <a href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/#c-language-extensions">C language extensions</a> appendix from the CUDA C programming guide. For more information about certain intrinsics, refer to the aforementioned NVIDIA documentation.</p><h2 id="Indexing-and-dimensions"><a class="docs-heading-anchor" href="#Indexing-and-dimensions">Indexing and dimensions</a><a id="Indexing-and-dimensions-1"></a><a class="docs-heading-anchor-permalink" href="#Indexing-and-dimensions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.gridDim" href="#CUDA.gridDim"><code>CUDA.gridDim</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">gridDim()::NamedTuple</code></pre><p>Returns the dimensions of the grid.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/indexing.jl#L66-L70">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.blockIdx" href="#CUDA.blockIdx"><code>CUDA.blockIdx</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">blockIdx()::NamedTuple</code></pre><p>Returns the block index within the grid.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/indexing.jl#L73-L77">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.blockDim" href="#CUDA.blockDim"><code>CUDA.blockDim</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">blockDim()::NamedTuple</code></pre><p>Returns the dimensions of the block.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/indexing.jl#L80-L84">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.threadIdx" href="#CUDA.threadIdx"><code>CUDA.threadIdx</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">threadIdx()::NamedTuple</code></pre><p>Returns the thread index within the block.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/indexing.jl#L87-L91">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.warpsize" href="#CUDA.warpsize"><code>CUDA.warpsize</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">warpsize(dev::CuDevice)</code></pre><p>Returns the warp size (in threads) of the device.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/lib/cudadrv/devices.jl#L172-L176">source</a></section><section><div><pre><code class="language-julia hljs">warpsize()::Int32</code></pre><p>Returns the warp size (in threads).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/indexing.jl#L94-L98">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.laneid" href="#CUDA.laneid"><code>CUDA.laneid</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">laneid()::Int32</code></pre><p>Returns the thread&#39;s lane within the warp.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/indexing.jl#L101-L105">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.active_mask" href="#CUDA.active_mask"><code>CUDA.active_mask</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">active_mask()</code></pre><p>Returns a 32-bit mask indicating which threads in a warp are active with the current executing thread.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/indexing.jl#L130-L135">source</a></section></article><h2 id="Device-arrays"><a class="docs-heading-anchor" href="#Device-arrays">Device arrays</a><a id="Device-arrays-1"></a><a class="docs-heading-anchor-permalink" href="#Device-arrays" title="Permalink"></a></h2><p>CUDA.jl provides a primitive, lightweight array type to manage GPU data organized in an plain, dense fashion. This is the device-counterpart to the <code>CuArray</code>, and implements (part of) the array interface as well as other functionality for use <em>on</em> the GPU:</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.CuDeviceArray" href="#CUDA.CuDeviceArray"><code>CUDA.CuDeviceArray</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">CuDeviceArray{T,N,A}(ptr, dims, [maxsize])</code></pre><p>Construct an <code>N</code>-dimensional dense CUDA device array with element type <code>T</code> wrapping a pointer, where <code>N</code> is determined from the length of <code>dims</code> and <code>T</code> is determined from the type of <code>ptr</code>. <code>dims</code> may be a single scalar, or a tuple of integers corresponding to the lengths in each dimension). If the rank <code>N</code> is supplied explicitly as in <code>Array{T,N}(dims)</code>, then it must match the length of <code>dims</code>. The same applies to the element type <code>T</code>, which should match the type of the pointer <code>ptr</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/array.jl#L8-L17">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.Const" href="#CUDA.Const"><code>CUDA.Const</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">Const(A::CuDeviceArray)</code></pre><p>Mark a CuDeviceArray as constant/read-only. The invariant guaranteed is that you will not modify an CuDeviceArray for the duration of the current kernel.</p><p>This API can only be used on devices with compute capability 3.5 or higher.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>Experimental API. Subject to change without deprecation.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/array.jl#L196-L206">source</a></section></article><h2 id="Memory-types"><a class="docs-heading-anchor" href="#Memory-types">Memory types</a><a id="Memory-types-1"></a><a class="docs-heading-anchor-permalink" href="#Memory-types" title="Permalink"></a></h2><h3 id="Shared-memory"><a class="docs-heading-anchor" href="#Shared-memory">Shared memory</a><a id="Shared-memory-1"></a><a class="docs-heading-anchor-permalink" href="#Shared-memory" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.CuStaticSharedArray" href="#CUDA.CuStaticSharedArray"><code>CUDA.CuStaticSharedArray</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">CuStaticSharedArray(T::Type, dims) -&gt; CuDeviceArray{T,N,AS.Shared}</code></pre><p>Get an array of type <code>T</code> and dimensions <code>dims</code> (either an integer length or tuple shape) pointing to a statically-allocated piece of shared memory. The type should be statically inferable and the dimensions should be constant, or an error will be thrown and the generator function will be called dynamically.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/shared_memory.jl#L5-L12">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.CuDynamicSharedArray" href="#CUDA.CuDynamicSharedArray"><code>CUDA.CuDynamicSharedArray</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">CuDynamicSharedArray(T::Type, dims, offset::Integer=0) -&gt; CuDeviceArray{T,N,AS.Shared}</code></pre><p>Get an array of type <code>T</code> and dimensions <code>dims</code> (either an integer length or tuple shape) pointing to a dynamically-allocated piece of shared memory. The type should be statically inferable or an error will be thrown and the generator function will be called dynamically.</p><p>Note that the amount of dynamic shared memory needs to specified when launching the kernel.</p><p>Optionally, an offset parameter indicating how many bytes to add to the base shared memory pointer can be specified. This is useful when dealing with a heterogeneous buffer of dynamic shared memory; in the case of a homogeneous multi-part buffer it is preferred to use <code>view</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/shared_memory.jl#L30-L42">source</a></section></article><h3 id="Texture-memory"><a class="docs-heading-anchor" href="#Texture-memory">Texture memory</a><a id="Texture-memory-1"></a><a class="docs-heading-anchor-permalink" href="#Texture-memory" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.CuDeviceTexture" href="#CUDA.CuDeviceTexture"><code>CUDA.CuDeviceTexture</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">CuDeviceTexture{T,N,M,NC,I}</code></pre><p><code>N</code>-dimensional device texture with elements of type <code>T</code>. This type is the device-side counterpart of <a href="../../lib/driver/#CUDA.CuTexture"><code>CuTexture{T,N,P}</code></a>, and can be used to access textures using regular indexing notation. If <code>NC</code> is true, indices used by these accesses should be normalized, i.e., fall into the <code>[0,1)</code> domain. The <code>I</code> type parameter indicates the kind of interpolation that happens when indexing into this texture. The source memory of the texture is specified by the <code>M</code> parameter, either linear memory or a texture array.</p><p>Device-side texture objects cannot be created directly, but should be created host-side using <a href="../../lib/driver/#CUDA.CuTexture"><code>CuTexture{T,N,P}</code></a> and passed to the kernel as an argument.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>Experimental API. Subject to change without deprecation.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/texture.jl#L12-L27">source</a></section></article><h2 id="Synchronization"><a class="docs-heading-anchor" href="#Synchronization">Synchronization</a><a id="Synchronization-1"></a><a class="docs-heading-anchor-permalink" href="#Synchronization" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.sync_threads" href="#CUDA.sync_threads"><code>CUDA.sync_threads</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">sync_threads()</code></pre><p>Waits until all threads in the thread block have reached this point and all global and shared memory accesses made by these threads prior to <code>sync_threads()</code> are visible to all threads in the block.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/synchronization.jl#L9-L15">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.sync_threads_count" href="#CUDA.sync_threads_count"><code>CUDA.sync_threads_count</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">sync_threads_count(predicate)</code></pre><p>Identical to <code>sync_threads()</code> with the additional feature that it evaluates predicate for all threads of the block and returns the number of threads for which <code>predicate</code> evaluates to true.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/synchronization.jl#L18-L24">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.sync_threads_and" href="#CUDA.sync_threads_and"><code>CUDA.sync_threads_and</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">sync_threads_and(predicate)</code></pre><p>Identical to <code>sync_threads()</code> with the additional feature that it evaluates predicate for all threads of the block and returns <code>true</code> if and only if <code>predicate</code> evaluates to <code>true</code> for all of them.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/synchronization.jl#L28-L34">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.sync_threads_or" href="#CUDA.sync_threads_or"><code>CUDA.sync_threads_or</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">sync_threads_or(predicate)</code></pre><p>Identical to <code>sync_threads()</code> with the additional feature that it evaluates predicate for all threads of the block and returns <code>true</code> if and only if <code>predicate</code> evaluates to <code>true</code> for any of them.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/synchronization.jl#L38-L44">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.sync_warp" href="#CUDA.sync_warp"><code>CUDA.sync_warp</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">sync_warp(mask::Integer=FULL_MASK)</code></pre><p>Waits threads in the warp, selected by means of the bitmask <code>mask</code>, have reached this point and all global and shared memory accesses made by these threads prior to <code>sync_warp()</code> are visible to those threads in the warp. The default value for <code>mask</code> selects all threads in the warp.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Requires CUDA &gt;= 9.0 and sm_6.2</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/synchronization.jl#L48-L58">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.threadfence_block" href="#CUDA.threadfence_block"><code>CUDA.threadfence_block</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">threadfence_block()</code></pre><p>A memory fence that ensures that:</p><ul><li>All writes to all memory made by the calling thread before the call to <code>threadfence_block()</code> are observed by all threads in the block of the calling thread as occurring before all writes to all memory made by the calling thread after the call to <code>threadfence_block()</code></li><li>All reads from all memory made by the calling thread before the call to <code>threadfence_block()</code> are ordered before all reads from all memory made by the calling thread after the call to <code>threadfence_block()</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/synchronization.jl#L74-L83">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.threadfence" href="#CUDA.threadfence"><code>CUDA.threadfence</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">threadfence()</code></pre><p>A memory fence that acts as <a href="#CUDA.threadfence_block"><code>threadfence_block</code></a> for all threads in the block of the calling thread and also ensures that no writes to all memory made by the calling thread after the call to <code>threadfence()</code> are observed by any thread in the device as occurring before any write to all memory made by the calling thread before the call to <code>threadfence()</code>.</p><p>Note that for this ordering guarantee to be true, the observing threads must truly observe the memory and not cached versions of it; this is requires the use of volatile loads and stores, which is not available from Julia right now.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/synchronization.jl#L86-L97">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.threadfence_system" href="#CUDA.threadfence_system"><code>CUDA.threadfence_system</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">threadfence_system()</code></pre><p>A memory fence that acts as <a href="#CUDA.threadfence_block"><code>threadfence_block</code></a> for all threads in the block of the calling thread and also ensures that all writes to all memory made by the calling thread before the call to <code>threadfence_system()</code> are observed by all threads in the device, host threads, and all threads in peer devices as occurring before all writes to all memory made by the calling thread after the call to <code>threadfence_system()</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/synchronization.jl#L100-L108">source</a></section></article><h2 id="Time-functions"><a class="docs-heading-anchor" href="#Time-functions">Time functions</a><a id="Time-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Time-functions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.clock" href="#CUDA.clock"><code>CUDA.clock</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">clock(UInt32)</code></pre><p>Returns the value of a per-multiprocessor counter that is incremented every clock cycle.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/misc.jl#L10-L14">source</a></section><section><div><pre><code class="language-julia hljs">clock(UInt64)</code></pre><p>Returns the value of a per-multiprocessor counter that is incremented every clock cycle.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/misc.jl#L17-L21">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.nanosleep" href="#CUDA.nanosleep"><code>CUDA.nanosleep</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">nanosleep(t)</code></pre><p>Puts a thread for a given amount <code>t</code>(in nanoseconds).</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Requires CUDA &gt;= 10.0 and sm_6.2</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/misc.jl#L25-L32">source</a></section></article><h2 id="Warp-level-functions"><a class="docs-heading-anchor" href="#Warp-level-functions">Warp-level functions</a><a id="Warp-level-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Warp-level-functions" title="Permalink"></a></h2><h3 id="Voting"><a class="docs-heading-anchor" href="#Voting">Voting</a><a id="Voting-1"></a><a class="docs-heading-anchor-permalink" href="#Voting" title="Permalink"></a></h3><p>The warp vote functions allow the threads of a given warp to perform a reduction-and-broadcast operation. These functions take as input a boolean predicate from each thread in the warp and evaluate it. The results of that evaluation are combined (reduced) across the active threads of the warp in one different ways, broadcasting a single return value to each participating thread.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.vote_all_sync" href="#CUDA.vote_all_sync"><code>CUDA.vote_all_sync</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">vote_all_sync(mask::UInt32, predicate::Bool)</code></pre><p>Evaluate <code>predicate</code> for all active threads of the warp and return whether <code>predicate</code> is true for all of them.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/warp.jl#L126-L131">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.vote_any_sync" href="#CUDA.vote_any_sync"><code>CUDA.vote_any_sync</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">vote_any_sync(mask::UInt32, predicate::Bool)</code></pre><p>Evaluate <code>predicate</code> for all active threads of the warp and return whether <code>predicate</code> is true for any of them.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/warp.jl#L134-L139">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.vote_uni_sync" href="#CUDA.vote_uni_sync"><code>CUDA.vote_uni_sync</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">vote_uni_sync(mask::UInt32, predicate::Bool)</code></pre><p>Evaluate <code>predicate</code> for all active threads of the warp and return whether <code>predicate</code> is the same for any of them.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/warp.jl#L142-L147">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.vote_ballot_sync" href="#CUDA.vote_ballot_sync"><code>CUDA.vote_ballot_sync</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">vote_ballot_sync(mask::UInt32, predicate::Bool)</code></pre><p>Evaluate <code>predicate</code> for all active threads of the warp and return an integer whose Nth bit is set if and only if <code>predicate</code> is true for the Nth thread of the warp and the Nth thread is active.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/warp.jl#L150-L156">source</a></section></article><h3 id="Shuffle"><a class="docs-heading-anchor" href="#Shuffle">Shuffle</a><a id="Shuffle-1"></a><a class="docs-heading-anchor-permalink" href="#Shuffle" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.shfl_sync" href="#CUDA.shfl_sync"><code>CUDA.shfl_sync</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">shfl_sync(threadmask::UInt32, val, lane::Integer, width::Integer=32)</code></pre><p>Shuffle a value from a directly indexed lane <code>lane</code>, and synchronize threads according to <code>threadmask</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/warp.jl#L37-L42">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.shfl_up_sync" href="#CUDA.shfl_up_sync"><code>CUDA.shfl_up_sync</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">shfl_up_sync(threadmask::UInt32, val, delta::Integer, width::Integer=32)</code></pre><p>Shuffle a value from a lane with lower ID relative to caller, and synchronize threads according to <code>threadmask</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/warp.jl#L44-L49">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.shfl_down_sync" href="#CUDA.shfl_down_sync"><code>CUDA.shfl_down_sync</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">shfl_down_sync(threadmask::UInt32, val, delta::Integer, width::Integer=32)</code></pre><p>Shuffle a value from a lane with higher ID relative to caller, and synchronize threads according to <code>threadmask</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/warp.jl#L51-L56">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.shfl_xor_sync" href="#CUDA.shfl_xor_sync"><code>CUDA.shfl_xor_sync</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">shfl_xor_sync(threadmask::UInt32, val, mask::Integer, width::Integer=32)</code></pre><p>Shuffle a value from a lane based on bitwise XOR of own lane ID with <code>mask</code>, and synchronize threads according to <code>threadmask</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/warp.jl#L58-L63">source</a></section></article><h2 id="Formatted-Output"><a class="docs-heading-anchor" href="#Formatted-Output">Formatted Output</a><a id="Formatted-Output-1"></a><a class="docs-heading-anchor-permalink" href="#Formatted-Output" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.@cushow" href="#CUDA.@cushow"><code>CUDA.@cushow</code></a> — <span class="docstring-category">Macro</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">@cushow(ex)</code></pre><p>GPU analog of <code>Base.@show</code>. It comes with the same type restrictions as <a href="#CUDA.@cuprintf"><code>@cuprintf</code></a>.</p><pre><code class="language-julia hljs">@cushow threadIdx().x</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/output.jl#L238-L246">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.@cuprint" href="#CUDA.@cuprint"><code>CUDA.@cuprint</code></a> — <span class="docstring-category">Macro</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">@cuprint(xs...)
@cuprintln(xs...)</code></pre><p>Print a textual representation of values <code>xs</code> to standard output from the GPU. The functionality builds on <code>@cuprintf</code>, and is intended as a more use friendly alternative of that API. However, that also means there&#39;s only limited support for argument types, handling 16/32/64 signed and unsigned integers, 32 and 64-bit floating point numbers, <code>Cchar</code>s and pointers. For more complex output, use <code>@cuprintf</code> directly.</p><p>Limited string interpolation is also possible:</p><pre><code class="language-julia hljs">    @cuprint(&quot;Hello, World &quot;, 42, &quot;\n&quot;)
    @cuprint &quot;Hello, World $(42)\n&quot;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/output.jl#L180-L196">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.@cuprintln" href="#CUDA.@cuprintln"><code>CUDA.@cuprintln</code></a> — <span class="docstring-category">Macro</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">@cuprint(xs...)
@cuprintln(xs...)</code></pre><p>Print a textual representation of values <code>xs</code> to standard output from the GPU. The functionality builds on <code>@cuprintf</code>, and is intended as a more use friendly alternative of that API. However, that also means there&#39;s only limited support for argument types, handling 16/32/64 signed and unsigned integers, 32 and 64-bit floating point numbers, <code>Cchar</code>s and pointers. For more complex output, use <code>@cuprintf</code> directly.</p><p>Limited string interpolation is also possible:</p><pre><code class="language-julia hljs">    @cuprint(&quot;Hello, World &quot;, 42, &quot;\n&quot;)
    @cuprint &quot;Hello, World $(42)\n&quot;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/output.jl#L180-L196">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.@cuprintf" href="#CUDA.@cuprintf"><code>CUDA.@cuprintf</code></a> — <span class="docstring-category">Macro</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">@cuprintf(&quot;%Fmt&quot;, args...)</code></pre><p>Print a formatted string in device context on the host standard output.</p><p>Note that this is not a fully C-compliant <code>printf</code> implementation; see the CUDA documentation for supported options and inputs.</p><p>Also beware that it is an untyped, and unforgiving <code>printf</code> implementation. Type widths need to match, eg. printing a 64-bit Julia integer requires the <code>%ld</code> formatting string.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/output.jl#L21-L31">source</a></section></article><h2 id="Assertions"><a class="docs-heading-anchor" href="#Assertions">Assertions</a><a id="Assertions-1"></a><a class="docs-heading-anchor-permalink" href="#Assertions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.@cuassert" href="#CUDA.@cuassert"><code>CUDA.@cuassert</code></a> — <span class="docstring-category">Macro</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">@assert cond [text]</code></pre><p>Signal assertion failure to the CUDA driver if <code>cond</code> is <code>false</code>. Preferred syntax for writing assertions, mimicking <code>Base.@assert</code>. Message <code>text</code> is optionally displayed upon assertion failure.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>A failed assertion will crash the GPU, so use sparingly as a debugging tool. Furthermore, the assertion might be disabled at various optimization levels, and thus should not cause any side-effects.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/assertion.jl#L5-L16">source</a></section></article><h2 id="Atomics"><a class="docs-heading-anchor" href="#Atomics">Atomics</a><a id="Atomics-1"></a><a class="docs-heading-anchor-permalink" href="#Atomics" title="Permalink"></a></h2><p>A high-level macro is available to annotate expressions with:</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.@atomic" href="#CUDA.@atomic"><code>CUDA.@atomic</code></a> — <span class="docstring-category">Macro</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">@atomic a[I] = op(a[I], val)
@atomic a[I] ...= val</code></pre><p>Atomically perform a sequence of operations that loads an array element <code>a[I]</code>, performs the operation <code>op</code> on that value and a second value <code>val</code>, and writes the result back to the array. This sequence can be written out as a regular assignment, in which case the same array element should be used in the left and right hand side of the assignment, or as an in-place application of a known operator. In both cases, the array reference should be pure and not induce any side-effects.</p><div class="admonition is-category-warn"><header class="admonition-header">Warn</header><div class="admonition-body"><p>This interface is experimental, and might change without warning.  Use the lower-level <code>atomic_...!</code> functions for a stable API, albeit one limited to natively-supported ops.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/atomics.jl#L395-L409">source</a></section></article><p>If your expression is not recognized, or you need more control, use the underlying functions:</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.atomic_cas!" href="#CUDA.atomic_cas!"><code>CUDA.atomic_cas!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">atomic_cas!(ptr::LLVMPtr{T}, cmp::T, val::T)</code></pre><p>Reads the value <code>old</code> located at address <code>ptr</code> and compare with <code>cmp</code>. If <code>old</code> equals to <code>cmp</code>, stores <code>val</code> at the same address. Otherwise, doesn&#39;t change the value <code>old</code>. These operations are performed in one atomic transaction. The function returns <code>old</code>.</p><p>This operation is supported for values of type Int32, Int64, UInt32 and UInt64. Additionally, on GPU hardware with compute capability 7.0+, values of type UInt16 are supported.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/atomics.jl#L236-L246">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.atomic_xchg!" href="#CUDA.atomic_xchg!"><code>CUDA.atomic_xchg!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">atomic_xchg!(ptr::LLVMPtr{T}, val::T)</code></pre><p>Reads the value <code>old</code> located at address <code>ptr</code> and stores <code>val</code> at the same address. These operations are performed in one atomic transaction. The function returns <code>old</code>.</p><p>This operation is supported for values of type Int32, Int64, UInt32 and UInt64.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/atomics.jl#L249-L256">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.atomic_add!" href="#CUDA.atomic_add!"><code>CUDA.atomic_add!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">atomic_add!(ptr::LLVMPtr{T}, val::T)</code></pre><p>Reads the value <code>old</code> located at address <code>ptr</code>, computes <code>old + val</code>, and stores the result back to memory at the same address. These operations are performed in one atomic transaction. The function returns <code>old</code>.</p><p>This operation is supported for values of type Int32, Int64, UInt32, UInt64, and Float32. Additionally, on GPU hardware with compute capability 6.0+, values of type Float64 are supported.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/atomics.jl#L259-L269">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.atomic_sub!" href="#CUDA.atomic_sub!"><code>CUDA.atomic_sub!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">atomic_sub!(ptr::LLVMPtr{T}, val::T)</code></pre><p>Reads the value <code>old</code> located at address <code>ptr</code>, computes <code>old - val</code>, and stores the result back to memory at the same address. These operations are performed in one atomic transaction. The function returns <code>old</code>.</p><p>This operation is supported for values of type Int32, Int64, UInt32 and UInt64.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/atomics.jl#L272-L280">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.atomic_and!" href="#CUDA.atomic_and!"><code>CUDA.atomic_and!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">atomic_and!(ptr::LLVMPtr{T}, val::T)</code></pre><p>Reads the value <code>old</code> located at address <code>ptr</code>, computes <code>old &amp; val</code>, and stores the result back to memory at the same address. These operations are performed in one atomic transaction. The function returns <code>old</code>.</p><p>This operation is supported for values of type Int32, Int64, UInt32 and UInt64.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/atomics.jl#L283-L291">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.atomic_or!" href="#CUDA.atomic_or!"><code>CUDA.atomic_or!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">atomic_or!(ptr::LLVMPtr{T}, val::T)</code></pre><p>Reads the value <code>old</code> located at address <code>ptr</code>, computes <code>old | val</code>, and stores the result back to memory at the same address. These operations are performed in one atomic transaction. The function returns <code>old</code>.</p><p>This operation is supported for values of type Int32, Int64, UInt32 and UInt64.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/atomics.jl#L294-L302">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.atomic_xor!" href="#CUDA.atomic_xor!"><code>CUDA.atomic_xor!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">atomic_xor!(ptr::LLVMPtr{T}, val::T)</code></pre><p>Reads the value <code>old</code> located at address <code>ptr</code>, computes <code>old ⊻ val</code>, and stores the result back to memory at the same address. These operations are performed in one atomic transaction. The function returns <code>old</code>.</p><p>This operation is supported for values of type Int32, Int64, UInt32 and UInt64.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/atomics.jl#L305-L313">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.atomic_min!" href="#CUDA.atomic_min!"><code>CUDA.atomic_min!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">atomic_min!(ptr::LLVMPtr{T}, val::T)</code></pre><p>Reads the value <code>old</code> located at address <code>ptr</code>, computes <code>min(old, val)</code>, and stores the result back to memory at the same address. These operations are performed in one atomic transaction. The function returns <code>old</code>.</p><p>This operation is supported for values of type Int32, Int64, UInt32 and UInt64.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/atomics.jl#L316-L324">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.atomic_max!" href="#CUDA.atomic_max!"><code>CUDA.atomic_max!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">atomic_max!(ptr::LLVMPtr{T}, val::T)</code></pre><p>Reads the value <code>old</code> located at address <code>ptr</code>, computes <code>max(old, val)</code>, and stores the result back to memory at the same address. These operations are performed in one atomic transaction. The function returns <code>old</code>.</p><p>This operation is supported for values of type Int32, Int64, UInt32 and UInt64.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/atomics.jl#L327-L335">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.atomic_inc!" href="#CUDA.atomic_inc!"><code>CUDA.atomic_inc!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">atomic_inc!(ptr::LLVMPtr{T}, val::T)</code></pre><p>Reads the value <code>old</code> located at address <code>ptr</code>, computes <code>((old &gt;= val) ? 0 : (old+1))</code>, and stores the result back to memory at the same address. These three operations are performed in one atomic transaction. The function returns <code>old</code>.</p><p>This operation is only supported for values of type Int32.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/atomics.jl#L338-L346">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.atomic_dec!" href="#CUDA.atomic_dec!"><code>CUDA.atomic_dec!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">atomic_dec!(ptr::LLVMPtr{T}, val::T)</code></pre><p>Reads the value <code>old</code> located at address <code>ptr</code>, computes <code>(((old == 0) | (old &gt; val)) ? val : (old-1) )</code>, and stores the result back to memory at the same address. These three operations are performed in one atomic transaction. The function returns <code>old</code>.</p><p>This operation is only supported for values of type Int32.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/atomics.jl#L349-L357">source</a></section></article><h2 id="Dynamic-parallelism"><a class="docs-heading-anchor" href="#Dynamic-parallelism">Dynamic parallelism</a><a id="Dynamic-parallelism-1"></a><a class="docs-heading-anchor-permalink" href="#Dynamic-parallelism" title="Permalink"></a></h2><p>Similarly to launching kernels from the host, you can use <code>@cuda</code> while passing <code>dynamic=true</code> for launching kernels from the device. A lower-level API is available as well:</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.dynamic_cufunction" href="#CUDA.dynamic_cufunction"><code>CUDA.dynamic_cufunction</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">dynamic_cufunction(f, tt=Tuple{})</code></pre><p>Low-level interface to compile a function invocation for the currently-active GPU, returning a callable kernel object. Device-side equivalent of <a href="../compiler/#CUDA.cufunction"><code>CUDA.cufunction</code></a>.</p><p>No keyword arguments are supported.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/compiler/execution.jl#L409-L416">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.DeviceKernel" href="#CUDA.DeviceKernel"><code>CUDA.DeviceKernel</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">(::HostKernel)(args...; kwargs...)
(::DeviceKernel)(args...; kwargs...)</code></pre><p>Low-level interface to call a compiled kernel, passing GPU-compatible arguments in <code>args</code>. For a higher-level interface, use <a href="../compiler/#CUDA.@cuda"><code>@cuda</code></a>.</p><p>A <code>HostKernel</code> is callable on the host, and a <code>DeviceKernel</code> is callable on the device (created by <code>@cuda</code> with <code>dynamic=true</code>).</p><p>The following keyword arguments are supported:</p><ul><li><code>threads</code> (default: <code>1</code>): Number of threads per block, or a 1-, 2- or 3-tuple of dimensions (e.g. <code>threads=(32, 32)</code> for a 2D block of 32×32 threads). Use <a href="#CUDA.threadIdx"><code>threadIdx()</code></a> and <a href="#CUDA.blockDim"><code>blockDim()</code></a> to query from within the kernel.</li><li><code>blocks</code> (default: <code>1</code>): Number of thread blocks to launch, or a 1-, 2- or 3-tuple of dimensions (e.g. <code>blocks=(2, 4, 2)</code> for a 3D grid of blocks). Use <a href="#CUDA.blockIdx"><code>blockIdx()</code></a> and <a href="#CUDA.gridDim"><code>gridDim()</code></a> to query from within the kernel.</li><li><code>shmem</code>(default: <code>0</code>): Amount of dynamic shared memory in bytes to allocate per thread block; used by <a href="#CUDA.CuDynamicSharedArray"><code>CuDynamicSharedArray</code></a>.</li><li><code>stream</code> (default: <a href="../essentials/#CUDA.stream"><code>stream()</code></a>): <a href="../../lib/driver/#CUDA.CuStream"><code>CuStream</code></a> to launch the kernel on.</li><li><code>cooperative</code> (default: <code>false</code>): whether to launch a cooperative kernel that supports grid synchronization (see <a href="#CUDA.CG.this_grid"><code>CG.this_grid</code></a> and <a href="#CUDA.CG.sync"><code>CG.sync</code></a>). Note that this requires care wrt. the number of blocks launched.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/compiler/execution.jl#L210-L233">source</a></section></article><h2 id="Cooperative-groups"><a class="docs-heading-anchor" href="#Cooperative-groups">Cooperative groups</a><a id="Cooperative-groups-1"></a><a class="docs-heading-anchor-permalink" href="#Cooperative-groups" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.CG" href="#CUDA.CG"><code>CUDA.CG</code></a> — <span class="docstring-category">Module</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>CUDA.jl&#39;s cooperative groups implementation.</p><p>Cooperative groups in CUDA offer a structured approach to synchronize and communicate among threads. They allow developers to define specific groups of threads, providing a means to fine-tune inter-thread communication granularity. By offering a more nuanced alternative to traditional CUDA synchronization methods, cooperative groups enable a more controlled and efficient parallel decomposition in kernel design.</p><p>The following functionality is available in CUDA.jl:</p><ul><li>implicit groups: thread blocks, grid groups, and coalesced groups.</li><li>synchronization: <code>sync</code>, <code>barrier_arrive</code>, <code>barrier_wait</code></li><li>warp collectives for coalesced groups: shuffle and voting</li><li>data transfer: <code>memcpy_async</code>, <code>wait</code> and <code>wait_prior</code></li></ul><p>Noteworthy missing functionality:</p><ul><li>implicit groups: clusters, and multi-grid groups (which are deprecated)</li><li>explicit groups: tiling and partitioning</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/cooperative_groups.jl#L3-L23">source</a></section></article><h3 id="Group-construction-and-properties"><a class="docs-heading-anchor" href="#Group-construction-and-properties">Group construction and properties</a><a id="Group-construction-and-properties-1"></a><a class="docs-heading-anchor-permalink" href="#Group-construction-and-properties" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.CG.thread_rank" href="#CUDA.CG.thread_rank"><code>CUDA.CG.thread_rank</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">thread_rank(group)</code></pre><p>Returns the linearized rank of the calling thread along the interval <code>[1, num_threads()]</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/cooperative_groups.jl#L94-L98">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.CG.num_threads" href="#CUDA.CG.num_threads"><code>CUDA.CG.num_threads</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">num_threads(group)</code></pre><p>Returns the total number of threads in the group.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/cooperative_groups.jl#L101-L105">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.CG.thread_block" href="#CUDA.CG.thread_block"><code>CUDA.CG.thread_block</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">thread_block &lt;: thread_group</code></pre><p>Every GPU kernel is executed by a grid of thread blocks, and threads within each block are guaranteed to reside on the same streaming multiprocessor. A <code>thread_block</code> represents a thread block whose dimensions are not known until runtime.</p><p>Constructed via <a href="#CUDA.CG.this_thread_block"><code>this_thread_block</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/cooperative_groups.jl#L113-L121">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.CG.this_thread_block" href="#CUDA.CG.this_thread_block"><code>CUDA.CG.this_thread_block</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">this_thread_block()</code></pre><p>Constructs a <code>thread_block</code> group</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/cooperative_groups.jl#L125-L129">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.CG.group_index" href="#CUDA.CG.group_index"><code>CUDA.CG.group_index</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">group_index(tb::thread_block)</code></pre><p>3-Dimensional index of the block within the launched grid.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/cooperative_groups.jl#L134-L138">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.CG.thread_index" href="#CUDA.CG.thread_index"><code>CUDA.CG.thread_index</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">thread_index(tb::thread_block)</code></pre><p>3-Dimensional index of the thread within the launched block.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/cooperative_groups.jl#L141-L145">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.CG.dim_threads" href="#CUDA.CG.dim_threads"><code>CUDA.CG.dim_threads</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">dim_threads(tb::thread_block)</code></pre><p>Dimensions of the launched block in units of threads.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/cooperative_groups.jl#L148-L152">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.CG.grid_group" href="#CUDA.CG.grid_group"><code>CUDA.CG.grid_group</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">grid_group &lt;: thread_group</code></pre><p>Threads within this this group are guaranteed to be co-resident on the same device within the same launched kernel. To use this group, the kernel must have been launched with <code>@cuda cooperative=true</code>, and the device must support it (queryable device attribute).</p><p>Constructed via <a href="#CUDA.CG.this_grid"><code>this_grid</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/cooperative_groups.jl#L163-L171">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.CG.this_grid" href="#CUDA.CG.this_grid"><code>CUDA.CG.this_grid</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">this_grid()</code></pre><p>Constructs a <code>grid_group</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/cooperative_groups.jl#L176-L180">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.CG.is_valid" href="#CUDA.CG.is_valid"><code>CUDA.CG.is_valid</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">is_valid(gg::grid_group)</code></pre><p>Returns whether the grid_group can synchronize</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/cooperative_groups.jl#L191-L195">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.CG.block_rank" href="#CUDA.CG.block_rank"><code>CUDA.CG.block_rank</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">block_rank(gg::grid_group)</code></pre><p>Rank of the calling block within [0, num_blocks)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/cooperative_groups.jl#L203-L207">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.CG.num_blocks" href="#CUDA.CG.num_blocks"><code>CUDA.CG.num_blocks</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">num_blocks(gg::grid_group)</code></pre><p>Total number of blocks in the group.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/cooperative_groups.jl#L215-L219">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.CG.dim_blocks" href="#CUDA.CG.dim_blocks"><code>CUDA.CG.dim_blocks</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">dim_blocks(gg::grid_group)</code></pre><p>Dimensions of the launched grid in units of blocks.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/cooperative_groups.jl#L225-L229">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.CG.block_index" href="#CUDA.CG.block_index"><code>CUDA.CG.block_index</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">block_index(gg::grid_group)</code></pre><p>3-Dimensional index of the block within the launched grid.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/cooperative_groups.jl#L232-L236">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.CG.coalesced_group" href="#CUDA.CG.coalesced_group"><code>CUDA.CG.coalesced_group</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">coalesced_group &lt;: thread_group</code></pre><p>A group representing the current set of converged threads in a warp. The size of the group is not guaranteed and it may return a group of only one thread (itself).</p><p>This group exposes warp-synchronous builtins. Constructed via <a href="#CUDA.CG.coalesced_threads"><code>coalesced_threads</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/cooperative_groups.jl#L244-L251">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.CG.coalesced_threads" href="#CUDA.CG.coalesced_threads"><code>CUDA.CG.coalesced_threads</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">coalesced_threads()</code></pre><p>Constructs a <code>coalesced_group</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/cooperative_groups.jl#L261-L265">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.CG.meta_group_rank" href="#CUDA.CG.meta_group_rank"><code>CUDA.CG.meta_group_rank</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">meta_group_rank(cg::coalesced_group)</code></pre><p>Rank of this group in the upper level of the hierarchy.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/cooperative_groups.jl#L272-L276">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.CG.meta_group_size" href="#CUDA.CG.meta_group_size"><code>CUDA.CG.meta_group_size</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">meta_group_size(cg::coalesced_group)</code></pre><p>Total number of partitions created out of all CTAs when the group was created.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/cooperative_groups.jl#L279-L283">source</a></section></article><h3 id="Synchronization-2"><a class="docs-heading-anchor" href="#Synchronization-2">Synchronization</a><a class="docs-heading-anchor-permalink" href="#Synchronization-2" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.CG.sync" href="#CUDA.CG.sync"><code>CUDA.CG.sync</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">sync(group)</code></pre><p>Synchronize the threads named in the group, equivalent to calling <a href="#CUDA.CG.barrier_wait"><code>barrier_wait</code></a> and <a href="#CUDA.CG.barrier_arrive"><code>barrier_arrive</code></a> in sequence.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/cooperative_groups.jl#L294-L299">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.CG.barrier_arrive" href="#CUDA.CG.barrier_arrive"><code>CUDA.CG.barrier_arrive</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">barrier_arrive(group)</code></pre><p>Arrive on the barrier, returns a token that needs to be passed into <a href="#CUDA.CG.barrier_wait"><code>barrier_wait</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/cooperative_groups.jl#L302-L306">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.CG.barrier_wait" href="#CUDA.CG.barrier_wait"><code>CUDA.CG.barrier_wait</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">barrier_wait(group, token)</code></pre><p>Wait on the barrier, takes arrival token returned from <a href="#CUDA.CG.barrier_arrive"><code>barrier_arrive</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/cooperative_groups.jl#L309-L313">source</a></section></article><h2 id="Data-transfer"><a class="docs-heading-anchor" href="#Data-transfer">Data transfer</a><a id="Data-transfer-1"></a><a class="docs-heading-anchor-permalink" href="#Data-transfer" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.CG.wait" href="#CUDA.CG.wait"><code>CUDA.CG.wait</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">wait(group)</code></pre><p>Make all threads in this group wait for all previously submitted <a href="#CUDA.CG.memcpy_async"><code>memcpy_async</code></a> operations to complete.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/cooperative_groups.jl#L511-L516">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.CG.wait_prior" href="#CUDA.CG.wait_prior"><code>CUDA.CG.wait_prior</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">wait_prior(group, stage)</code></pre><p>Make all threads in this group wait for all but <code>stage</code> previously submitted <a href="#CUDA.CG.memcpy_async"><code>memcpy_async</code></a> operations to complete.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/cooperative_groups.jl#L522-L527">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.CG.memcpy_async" href="#CUDA.CG.memcpy_async"><code>CUDA.CG.memcpy_async</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">memcpy_async(group, dst, src, bytes)</code></pre><p>Perform a group-wide collective memory copy from <code>src</code> to <code>dst</code> of <code>bytes</code> bytes. This operation may be performed asynchronously, so you should <a href="#CUDA.CG.wait"><code>wait</code></a> or <a href="#CUDA.CG.wait_prior"><code>wait_prior</code></a> before using the data. It is only supported by thread blocks and coalesced groups.</p><p>For this operation to be performed asynchronously, the following conditions must be met:</p><ul><li>the source and destination memory should be aligned to 4, 8 or 16 bytes. this will be deduced from the datatype, but can also be specified explicitly using <a href="#CUDA.align"><code>CUDA.align</code></a>.</li><li>the source should be global memory, and the destination should be shared memory.</li><li>the device should have compute capability 8.0 or higher.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/cooperative_groups.jl#L535-L549">source</a></section></article><h2 id="Math"><a class="docs-heading-anchor" href="#Math">Math</a><a id="Math-1"></a><a class="docs-heading-anchor-permalink" href="#Math" title="Permalink"></a></h2><p>Many mathematical functions are provided by the <code>libdevice</code> library, and are wrapped by CUDA.jl. These functions are used to implement well-known functions from the Julia standard library and packages like SpecialFunctions.jl, e.g., calling the <code>cos</code> function will automatically use <code>__nv_cos</code> from <code>libdevice</code> if possible.</p><p>Some functions do not have a counterpart in the Julia ecosystem, those have to be called directly. For example, to call <code>__nv_logb</code> or <code>__nv_logbf</code> you use <code>CUDA.logb</code> in a kernel.</p><p>For a list of available functions, look at <code>src/device/intrinsics/math.jl</code>.</p><h2 id="WMMA"><a class="docs-heading-anchor" href="#WMMA">WMMA</a><a id="WMMA-1"></a><a class="docs-heading-anchor-permalink" href="#WMMA" title="Permalink"></a></h2><p>Warp matrix multiply-accumulate (WMMA) is a CUDA API to access Tensor Cores, a new hardware feature in Volta GPUs to perform mixed precision matrix multiply-accumulate operations. The interface is split in two levels, both available in the WMMA submodule: low level wrappers around the LLVM intrinsics, and a higher-level API similar to that of CUDA C.</p><h3 id="LLVM-Intrinsics"><a class="docs-heading-anchor" href="#LLVM-Intrinsics">LLVM Intrinsics</a><a id="LLVM-Intrinsics-1"></a><a class="docs-heading-anchor-permalink" href="#LLVM-Intrinsics" title="Permalink"></a></h3><h4 id="Load-matrix"><a class="docs-heading-anchor" href="#Load-matrix">Load matrix</a><a id="Load-matrix-1"></a><a class="docs-heading-anchor-permalink" href="#Load-matrix" title="Permalink"></a></h4><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.WMMA.llvm_wmma_load" href="#CUDA.WMMA.llvm_wmma_load"><code>CUDA.WMMA.llvm_wmma_load</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">WMMA.llvm_wmma_load_{matrix}_{layout}_{shape}_{addr_space}_stride_{elem_type}(src_addr, stride)</code></pre><p>Wrapper around the LLVM intrinsic <code>@llvm.nvvm.wmma.load.{matrix}.sync.{layout}.{shape}.{addr_space}.stride.{elem_type}</code>.</p><p><strong>Arguments</strong></p><ul><li><code>src_addr</code>: The memory address to load from.</li><li><code>stride</code>: The leading dimension of the matrix, in numbers of elements.</li></ul><p><strong>Placeholders</strong></p><ul><li><code>{matrix}</code>: The matrix to load. Can be <code>a</code>, <code>b</code> or <code>c</code>.</li><li><code>{layout}</code>: The storage layout for the matrix. Can be <code>row</code> or <code>col</code>, for row major (C style) or column major (Julia style), respectively.</li><li><code>{shape}</code>: The overall shape of the MAC operation. Valid values are <code>m16n16k16</code>, <code>m32n8k16</code>, and <code>m8n32k16</code>.</li><li><code>{addr_space}</code>: The address space of <code>src_addr</code>. Can be empty (generic addressing), <code>shared</code> or <code>global</code>.</li><li><code>{elem_type}</code>: The type of each element in the matrix. For <code>a</code> and <code>b</code> matrices, valid values are <code>u8</code> (byte unsigned integer),               <code>s8</code> (byte signed integer), and <code>f16</code> (half precision floating point). For <code>c</code> and <code>d</code> matrices, valid values are               <code>s32</code> (32-bit signed integer), <code>f16</code> (half precision floating point), and <code>f32</code> (full precision floating point).</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/wmma.jl#L152-L169">source</a></section></article><h4 id="Perform-multiply-accumulate"><a class="docs-heading-anchor" href="#Perform-multiply-accumulate">Perform multiply-accumulate</a><a id="Perform-multiply-accumulate-1"></a><a class="docs-heading-anchor-permalink" href="#Perform-multiply-accumulate" title="Permalink"></a></h4><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.WMMA.llvm_wmma_mma" href="#CUDA.WMMA.llvm_wmma_mma"><code>CUDA.WMMA.llvm_wmma_mma</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">WMMA.llvm_wmma_mma_{a_layout}_{b_layout}_{shape}_{d_elem_type}_{c_elem_type}(a, b, c) or
WMMA.llvm_wmma_mma_{a_layout}_{b_layout}_{shape}_{a_elem_type}(a, b, c)</code></pre><p>For floating point operations: wrapper around the LLVM intrinsic <code>@llvm.nvvm.wmma.mma.sync.{a_layout}.{b_layout}.{shape}.{d_elem_type}.{c_elem_type}</code> For all other operations: wrapper around the LLVM intrinsic <code>@llvm.nvvm.wmma.mma.sync.{a_layout}.{b_layout}.{shape}.{a_elem_type}</code></p><p><strong>Arguments</strong></p><ul><li><code>a</code>: The WMMA fragment corresponding to the matrix <span>$A$</span>.</li><li><code>b</code>: The WMMA fragment corresponding to the matrix <span>$B$</span>.</li><li><code>c</code>: The WMMA fragment corresponding to the matrix <span>$C$</span>.</li></ul><p><strong>Placeholders</strong></p><ul><li><code>{a_layout}</code>: The storage layout for matrix <span>$A$</span>. Can be <code>row</code> or <code>col</code>, for row major (C style) or column major (Julia style), respectively. Note that this must match the layout used in the load operation.</li><li><code>{b_layout}</code>: The storage layout for matrix <span>$B$</span>. Can be <code>row</code> or <code>col</code>, for row major (C style) or column major (Julia style), respectively. Note that this must match the layout used in the load operation.</li><li><code>{shape}</code>: The overall shape of the MAC operation. Valid values are <code>m16n16k16</code>, <code>m32n8k16</code>, and <code>m8n32k16</code>.</li><li><code>{a_elem_type}</code>: The type of each element in the <span>$A$</span> matrix. Valid values are <code>u8</code> (byte unsigned integer), <code>s8</code> (byte signed integer), and <code>f16</code> (half precision floating point).</li><li><code>{d_elem_type}</code>: The type of each element in the resultant <span>$D$</span> matrix. Valid values are <code>s32</code> (32-bit signed integer), <code>f16</code> (half precision floating point), and <code>f32</code> (full precision floating point).</li><li><code>{c_elem_type}</code>: The type of each element in the <span>$C$</span> matrix. Valid values are <code>s32</code> (32-bit signed integer), <code>f16</code> (half precision floating point), and <code>f32</code> (full precision floating point).</li></ul><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>Remember that the shape, type and layout of all operations (be it MMA, load or store) <strong>MUST</strong> match. Otherwise, the behaviour is undefined!</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/wmma.jl#L282-L306">source</a></section></article><h4 id="Store-matrix"><a class="docs-heading-anchor" href="#Store-matrix">Store matrix</a><a id="Store-matrix-1"></a><a class="docs-heading-anchor-permalink" href="#Store-matrix" title="Permalink"></a></h4><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.WMMA.llvm_wmma_store" href="#CUDA.WMMA.llvm_wmma_store"><code>CUDA.WMMA.llvm_wmma_store</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">WMMA.llvm_wmma_store_d_{layout}_{shape}_{addr_space}_stride_{elem_type}(dst_addr, data, stride)</code></pre><p>Wrapper around the LLVM intrinsic <code>@llvm.nvvm.wmma.store.d.sync.{layout}.{shape}.{addr_space}.stride.{elem_type}</code>.</p><p><strong>Arguments</strong></p><ul><li><code>dst_addr</code>: The memory address to store to.</li><li><code>data</code>: The <span>$D$</span> fragment to store.</li><li><code>stride</code>: The leading dimension of the matrix, in numbers of elements.</li></ul><p><strong>Placeholders</strong></p><ul><li><code>{layout}</code>: The storage layout for the matrix. Can be <code>row</code> or <code>col</code>, for row major (C style) or column major (Julia style), respectively.</li><li><code>{shape}</code>: The overall shape of the MAC operation. Valid values are <code>m16n16k16</code>, <code>m32n8k16</code>, and <code>m8n32k16</code>.</li><li><code>{addr_space}</code>: The address space of <code>src_addr</code>. Can be empty (generic addressing), <code>shared</code> or <code>global</code>.</li><li><code>{elem_type}</code>: The type of each element in the matrix. For <code>a</code> and <code>b</code> matrices, valid values are <code>u8</code> (byte unsigned integer),               <code>s8</code> (byte signed integer), and <code>f16</code> (half precision floating point). For <code>c</code> and <code>d</code> matrices, valid values are               <code>s32</code> (32-bit signed integer), <code>f16</code> (half precision floating point), and <code>f32</code> (full precision floating point).</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/wmma.jl#L216-L233">source</a></section></article><h3 id="CUDA-C-like-API"><a class="docs-heading-anchor" href="#CUDA-C-like-API">CUDA C-like API</a><a id="CUDA-C-like-API-1"></a><a class="docs-heading-anchor-permalink" href="#CUDA-C-like-API" title="Permalink"></a></h3><h4 id="Fragment"><a class="docs-heading-anchor" href="#Fragment">Fragment</a><a id="Fragment-1"></a><a class="docs-heading-anchor-permalink" href="#Fragment" title="Permalink"></a></h4><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.WMMA.RowMajor" href="#CUDA.WMMA.RowMajor"><code>CUDA.WMMA.RowMajor</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">WMMA.RowMajor</code></pre><p>Type that represents a matrix stored in row major (C style) order.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/wmma.jl#L415-L419">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.WMMA.ColMajor" href="#CUDA.WMMA.ColMajor"><code>CUDA.WMMA.ColMajor</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">WMMA.ColMajor</code></pre><p>Type that represents a matrix stored in column major (Julia style) order.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/wmma.jl#L422-L426">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.WMMA.Unspecified" href="#CUDA.WMMA.Unspecified"><code>CUDA.WMMA.Unspecified</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">WMMA.Unspecified</code></pre><p>Type that represents a matrix stored in an unspecified order.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>This storage format is not valid for all WMMA operations!</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/wmma.jl#L429-L437">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.WMMA.FragmentLayout" href="#CUDA.WMMA.FragmentLayout"><code>CUDA.WMMA.FragmentLayout</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">WMMA.FragmentLayout</code></pre><p>Abstract type that specifies the storage layout of a matrix.</p><p>Possible values are <a href="#CUDA.WMMA.RowMajor"><code>WMMA.RowMajor</code></a>, <a href="#CUDA.WMMA.ColMajor"><code>WMMA.ColMajor</code></a> and <a href="#CUDA.WMMA.Unspecified"><code>WMMA.Unspecified</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/wmma.jl#L406-L412">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.WMMA.Fragment" href="#CUDA.WMMA.Fragment"><code>CUDA.WMMA.Fragment</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">WMMA.Fragment</code></pre><p>Type that represents per-thread intermediate results of WMMA operations.</p><p>You can access individual elements using the <code>x</code> member or <code>[]</code> operator, but beware that the exact ordering of elements is unspecified.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/wmma.jl#L451-L457">source</a></section></article><h4 id="WMMA-configuration"><a class="docs-heading-anchor" href="#WMMA-configuration">WMMA configuration</a><a id="WMMA-configuration-1"></a><a class="docs-heading-anchor-permalink" href="#WMMA-configuration" title="Permalink"></a></h4><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.WMMA.Config" href="#CUDA.WMMA.Config"><code>CUDA.WMMA.Config</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">WMMA.Config{M, N, K, d_type}</code></pre><p>Type that contains all information for WMMA operations that cannot be inferred from the argument&#39;s types.</p><p>WMMA instructions calculate the matrix multiply-accumulate operation <span>$D = A \cdot B + C$</span>, where <span>$A$</span> is a <span>$M \times K$</span> matrix, <span>$B$</span> a <span>$K \times N$</span> matrix, and <span>$C$</span> and <span>$D$</span> are <span>$M \times N$</span> matrices.</p><p><code>d_type</code> refers to the type of the elements of matrix <span>$D$</span>, and can be either <code>Float16</code> or <code>Float32</code>.</p><p>All WMMA operations take a <code>Config</code> as their final argument.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; config = WMMA.Config{16, 16, 16, Float32}
CUDA.WMMA.Config{16, 16, 16, Float32}</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/wmma.jl#L476-L493">source</a></section></article><h4 id="Load-matrix-2"><a class="docs-heading-anchor" href="#Load-matrix-2">Load matrix</a><a class="docs-heading-anchor-permalink" href="#Load-matrix-2" title="Permalink"></a></h4><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.WMMA.load_a" href="#CUDA.WMMA.load_a"><code>CUDA.WMMA.load_a</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">WMMA.load_a(addr, stride, layout, config)
WMMA.load_b(addr, stride, layout, config)
WMMA.load_c(addr, stride, layout, config)</code></pre><p>Load the matrix <code>a</code>, <code>b</code> or <code>c</code> from the memory location indicated by <code>addr</code>, and return the resulting <a href="#CUDA.WMMA.Fragment"><code>WMMA.Fragment</code></a>.</p><p><strong>Arguments</strong></p><ul><li><code>addr</code>: The address to load the matrix from.</li><li><code>stride</code>: The leading dimension of the matrix pointed to by <code>addr</code>, specified in number of elements.</li><li><code>layout</code>: The storage layout of the matrix. Possible values are <a href="#CUDA.WMMA.RowMajor"><code>WMMA.RowMajor</code></a> and <a href="#CUDA.WMMA.ColMajor"><code>WMMA.ColMajor</code></a>.</li><li><code>config</code>: The WMMA configuration that should be used for loading this matrix. See <a href="#CUDA.WMMA.Config"><code>WMMA.Config</code></a>.</li></ul><p>See also: <a href="#CUDA.WMMA.Fragment"><code>WMMA.Fragment</code></a>, <a href="#CUDA.WMMA.FragmentLayout"><code>WMMA.FragmentLayout</code></a>, <a href="#CUDA.WMMA.Config"><code>WMMA.Config</code></a></p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>All threads in a warp <strong>MUST</strong> execute the load operation in lockstep, and have to use exactly the same arguments. Failure to do so will result in undefined behaviour.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/wmma.jl#L577-L596">source</a></section></article><p><code>WMMA.load_b</code> and <code>WMMA.load_c</code> have the same signature.</p><h4 id="Perform-multiply-accumulate-2"><a class="docs-heading-anchor" href="#Perform-multiply-accumulate-2">Perform multiply-accumulate</a><a class="docs-heading-anchor-permalink" href="#Perform-multiply-accumulate-2" title="Permalink"></a></h4><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.WMMA.mma" href="#CUDA.WMMA.mma"><code>CUDA.WMMA.mma</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">WMMA.mma(a, b, c, conf)</code></pre><p>Perform the matrix multiply-accumulate operation <span>$D = A \cdot B + C$</span>.</p><p><strong>Arguments</strong></p><ul><li><code>a</code>: The <a href="#CUDA.WMMA.Fragment"><code>WMMA.Fragment</code></a> corresponding to the matrix <span>$A$</span>.</li><li><code>b</code>: The <a href="#CUDA.WMMA.Fragment"><code>WMMA.Fragment</code></a> corresponding to the matrix <span>$B$</span>.</li><li><code>c</code>: The <a href="#CUDA.WMMA.Fragment"><code>WMMA.Fragment</code></a> corresponding to the matrix <span>$C$</span>.</li><li><code>conf</code>: The <a href="#CUDA.WMMA.Config"><code>WMMA.Config</code></a> that should be used in this WMMA operation.</li></ul><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>All threads in a warp <strong>MUST</strong> execute the <code>mma</code> operation in lockstep, and have to use exactly the same arguments. Failure to do so will result in undefined behaviour.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/wmma.jl#L631-L647">source</a></section></article><h4 id="Store-matrix-2"><a class="docs-heading-anchor" href="#Store-matrix-2">Store matrix</a><a class="docs-heading-anchor-permalink" href="#Store-matrix-2" title="Permalink"></a></h4><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.WMMA.store_d" href="#CUDA.WMMA.store_d"><code>CUDA.WMMA.store_d</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">WMMA.store_d(addr, d, stride, layout, config)</code></pre><p>Store the result matrix <code>d</code> to the memory location indicated by <code>addr</code>.</p><p><strong>Arguments</strong></p><ul><li><code>addr</code>: The address to store the matrix to.</li><li><code>d</code>: The <a href="#CUDA.WMMA.Fragment"><code>WMMA.Fragment</code></a> corresponding to the <code>d</code> matrix.</li><li><code>stride</code>: The leading dimension of the matrix pointed to by <code>addr</code>, specified in number of elements.</li><li><code>layout</code>: The storage layout of the matrix. Possible values are <a href="#CUDA.WMMA.RowMajor"><code>WMMA.RowMajor</code></a> and <a href="#CUDA.WMMA.ColMajor"><code>WMMA.ColMajor</code></a>.</li><li><code>config</code>: The WMMA configuration that should be used for storing this matrix. See <a href="#CUDA.WMMA.Config"><code>WMMA.Config</code></a>.</li></ul><p>See also: <a href="#CUDA.WMMA.Fragment"><code>WMMA.Fragment</code></a>, <a href="#CUDA.WMMA.FragmentLayout"><code>WMMA.FragmentLayout</code></a>, <a href="#CUDA.WMMA.Config"><code>WMMA.Config</code></a></p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>All threads in a warp <strong>MUST</strong> execute the <code>store</code> operation in lockstep, and have to use exactly the same arguments. Failure to do so will result in undefined behaviour.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/wmma.jl#L686-L704">source</a></section></article><h4 id="Fill-fragment"><a class="docs-heading-anchor" href="#Fill-fragment">Fill fragment</a><a id="Fill-fragment-1"></a><a class="docs-heading-anchor-permalink" href="#Fill-fragment" title="Permalink"></a></h4><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.WMMA.fill_c" href="#CUDA.WMMA.fill_c"><code>CUDA.WMMA.fill_c</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">WMMA.fill_c(value, config)</code></pre><p>Return a <a href="#CUDA.WMMA.Fragment"><code>WMMA.Fragment</code></a> filled with the value <code>value</code>.</p><p>This operation is useful if you want to implement a matrix multiplication (and thus want to set <span>$C = O$</span>).</p><p><strong>Arguments</strong></p><ul><li><code>value</code>: The value used to fill the fragment. Can be a <code>Float16</code> or <code>Float32</code>.</li><li><code>config</code>: The WMMA configuration that should be used for this WMMA operation. See <a href="#CUDA.WMMA.Config"><code>WMMA.Config</code></a>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/intrinsics/wmma.jl#L735-L745">source</a></section></article><h2 id="Other"><a class="docs-heading-anchor" href="#Other">Other</a><a id="Other-1"></a><a class="docs-heading-anchor-permalink" href="#Other" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CUDA.align" href="#CUDA.align"><code>CUDA.align</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">CUDA.align{N}(obj)</code></pre><p>Construct an aligned object, providing alignment information to APIs that require it.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGPU/CUDA.jl/blob/05b9e5ffc86a24b1db2878b8e78ea55f268b54ed/src/device/utils.jl#L81-L85">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../array/">« Array programming</a><a class="docs-footer-nextpage" href="../compiler/">Compiler »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.7.0 on <span class="colophon-date" title="Monday 21 April 2025 21:30">Monday 21 April 2025</span>. Using Julia version 1.11.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
