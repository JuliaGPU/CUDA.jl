<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Benchmarking &amp; profiling · CUDA.jl</title><meta name="title" content="Benchmarking &amp; profiling · CUDA.jl"/><meta property="og:title" content="Benchmarking &amp; profiling · CUDA.jl"/><meta property="twitter:title" content="Benchmarking &amp; profiling · CUDA.jl"/><meta name="description" content="Documentation for CUDA.jl."/><meta property="og:description" content="Documentation for CUDA.jl."/><meta property="twitter:description" content="Documentation for CUDA.jl."/><meta property="og:url" content="https://cuda.juliagpu.org/stable/development/profiling/"/><meta property="twitter:url" content="https://cuda.juliagpu.org/stable/development/profiling/"/><link rel="canonical" href="https://cuda.juliagpu.org/stable/development/profiling/"/><script async src="https://www.googletagmanager.com/gtag/js?id=UA-154489943-2"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-154489943-2', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="CUDA.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">CUDA.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/introduction/">Introduction</a></li><li><a class="tocitem" href="../../tutorials/custom_structs/">Using custom structs</a></li><li><a class="tocitem" href="../../tutorials/performance/">Performance Tips</a></li></ul></li><li><span class="tocitem">Installation</span><ul><li><a class="tocitem" href="../../installation/overview/">Overview</a></li><li><a class="tocitem" href="../../installation/conditional/">Conditional use</a></li><li><a class="tocitem" href="../../installation/troubleshooting/">Troubleshooting</a></li></ul></li><li><span class="tocitem">Usage</span><ul><li><a class="tocitem" href="../../usage/overview/">Overview</a></li><li><a class="tocitem" href="../../usage/workflow/">Workflow</a></li><li><a class="tocitem" href="../../usage/array/">Array programming</a></li><li><a class="tocitem" href="../../usage/memory/">Memory management</a></li><li><a class="tocitem" href="../../usage/multitasking/">Tasks and threads</a></li><li><a class="tocitem" href="../../usage/multigpu/">Multiple GPUs</a></li></ul></li><li><span class="tocitem">Development</span><ul><li class="is-active"><a class="tocitem" href>Benchmarking &amp; profiling</a><ul class="internal"><li><a class="tocitem" href="#Time-measurements"><span>Time measurements</span></a></li><li><a class="tocitem" href="#Application-profiling"><span>Application profiling</span></a></li><li><a class="tocitem" href="#Source-code-annotations"><span>Source-code annotations</span></a></li><li><a class="tocitem" href="#Compiler-options"><span>Compiler options</span></a></li></ul></li><li><a class="tocitem" href="../kernel/">Kernel programming</a></li><li><a class="tocitem" href="../troubleshooting/">Troubleshooting</a></li><li><a class="tocitem" href="../debugging/">Debugging</a></li></ul></li><li><span class="tocitem">API reference</span><ul><li><a class="tocitem" href="../../api/essentials/">Essentials</a></li><li><a class="tocitem" href="../../api/array/">Array programming</a></li><li><a class="tocitem" href="../../api/kernel/">Kernel programming</a></li><li><a class="tocitem" href="../../api/compiler/">Compiler</a></li></ul></li><li><span class="tocitem">Library reference</span><ul><li><a class="tocitem" href="../../lib/driver/">CUDA driver</a></li></ul></li><li><a class="tocitem" href="../../faq/">FAQ</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Development</a></li><li class="is-active"><a href>Benchmarking &amp; profiling</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Benchmarking &amp; profiling</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaGPU/CUDA.jl/blob/master/docs/src/development/profiling.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Benchmarking-and-profiling"><a class="docs-heading-anchor" href="#Benchmarking-and-profiling">Benchmarking &amp; profiling</a><a id="Benchmarking-and-profiling-1"></a><a class="docs-heading-anchor-permalink" href="#Benchmarking-and-profiling" title="Permalink"></a></h1><p>Benchmarking and profiling a GPU program is harder than doing the same for a program executing on the CPU. For one, GPU operations typically execute asynchronously, and thus require appropriate synchronization when measuring their execution time. Furthermore, because the program executes on a different processor, it is much harder to know what is currently executing. CUDA, and the Julia CUDA packages, provide several tools and APIs to remedy this.</p><h2 id="Time-measurements"><a class="docs-heading-anchor" href="#Time-measurements">Time measurements</a><a id="Time-measurements-1"></a><a class="docs-heading-anchor-permalink" href="#Time-measurements" title="Permalink"></a></h2><p>To accurately measure execution time in the presence of asynchronously-executing GPU operations, CUDA.jl provides an <code>@elapsed</code> macro that, much like <code>Base.@elapsed</code>, measures the total execution time of a block of code on the GPU:</p><pre><code class="language-julia hljs">julia&gt; a = CUDA.rand(1024,1024,1024);

julia&gt; Base.@elapsed sin.(a)  # WRONG!
0.008714211

julia&gt; CUDA.@elapsed sin.(a)
0.051607586f0</code></pre><p>This is a low-level utility, and measures time by submitting events to the GPU and measuring the time between them. As such, if the GPU was not idle in the first place, you may not get the expected result. The macro is mainly useful if your application needs to know about the time it took to complete certain GPU operations.</p><p>For more convenient time reporting, you can use the <code>CUDA.@time</code> macro which mimics <code>Base.@time</code> by printing execution times as well as memory allocation stats, while making sure the GPU is idle before starting the measurement, as well as waiting for all asynchronous operations to complete:</p><pre><code class="language-julia hljs">julia&gt; a = CUDA.rand(1024,1024,1024);

julia&gt; CUDA.@time sin.(a);
  0.046063 seconds (96 CPU allocations: 3.750 KiB) (1 GPU allocation: 4.000 GiB, 14.33% gc time of which 99.89% spent allocating)</code></pre><p>The <code>CUDA.@time</code> macro is more user-friendly and is a generally more useful tool when measuring the end-to-end performance characteristics of a GPU application.</p><p>For robust measurements however, it is advised to use the <a href="https://github.com/JuliaCI/BenchmarkTools.jl">BenchmarkTools.jl</a> package which goes to great lengths to perform accurate measurements. Due to the asynchronous nature of GPUs, you need to ensure the GPU is synchronized at the end of every sample, e.g. by calling <code>synchronize()</code> or, even better, wrapping your code in <code>CUDA.@sync</code>:</p><pre><code class="language-julia hljs">julia&gt; a = CUDA.rand(1024,1024,1024);

julia&gt; @benchmark CUDA.@sync sin.($a)
BenchmarkTools.Trial:
  memory estimate:  3.73 KiB
  allocs estimate:  95
  --------------
  minimum time:     46.341 ms (0.00% GC)
  median time:      133.302 ms (0.50% GC)
  mean time:        130.087 ms (0.49% GC)
  maximum time:     153.465 ms (0.43% GC)
  --------------
  samples:          39
  evals/sample:     1</code></pre><p>Note that the allocations as reported by BenchmarkTools are CPU allocations. For the GPU allocation behavior you need to consult <code>CUDA.@time</code>.</p><h2 id="Application-profiling"><a class="docs-heading-anchor" href="#Application-profiling">Application profiling</a><a id="Application-profiling-1"></a><a class="docs-heading-anchor-permalink" href="#Application-profiling" title="Permalink"></a></h2><p>For profiling large applications, simple timings are insufficient. Instead, we want a overview of how and when the GPU was active, to avoid times where the device was idle and/or find which kernels needs optimization.</p><h3 id="Integrated-profiler"><a class="docs-heading-anchor" href="#Integrated-profiler">Integrated profiler</a><a id="Integrated-profiler-1"></a><a class="docs-heading-anchor-permalink" href="#Integrated-profiler" title="Permalink"></a></h3><p>Once again, we cannot use CPU utilities to profile GPU programs, as they will only paint a partial picture. Instead, CUDA.jl provides a <code>CUDA.@profile</code> macro that separately reports the time spent on the CPU, and the time spent on the GPU:</p><pre><code class="language-julia hljs">julia&gt; a = CUDA.rand(1024,1024,1024);

julia&gt; CUDA.@profile sin.(a)
Profiler ran for 11.93 ms, capturing 8 events.

Host-side activity: calling CUDA APIs took 437.26 µs (3.67% of the trace)
┌──────────┬───────────┬───────┬───────────┬───────────┬───────────┬─────────────────┐
│ Time (%) │      Time │ Calls │  Avg time │  Min time │  Max time │ Name            │
├──────────┼───────────┼───────┼───────────┼───────────┼───────────┼─────────────────┤
│    3.56% │ 424.15 µs │     1 │ 424.15 µs │ 424.15 µs │ 424.15 µs │ cuLaunchKernel  │
│    0.10% │  11.92 µs │     1 │  11.92 µs │  11.92 µs │  11.92 µs │ cuMemAllocAsync │
└──────────┴───────────┴───────┴───────────┴───────────┴───────────┴─────────────────┘

Device-side activity: GPU was busy for 11.48 ms (96.20% of the trace)
┌──────────┬──────────┬───────┬──────────┬──────────┬──────────┬───────────────────────
│ Time (%) │     Time │ Calls │ Avg time │ Min time │ Max time │ Name                 ⋯
├──────────┼──────────┼───────┼──────────┼──────────┼──────────┼───────────────────────
│   96.20% │ 11.48 ms │     1 │ 11.48 ms │ 11.48 ms │ 11.48 ms │ _Z16broadcast_kernel ⋯
└──────────┴──────────┴───────┴──────────┴──────────┴──────────┴───────────────────────</code></pre><p>By default, <code>CUDA.@profile</code> will provide a summary of host and device activities. If you prefer a chronological view of the events, you can set the <code>trace</code> keyword argument:</p><pre><code class="language-julia hljs">julia&gt; CUDA.@profile trace=true sin.(a)
Profiler ran for 11.71 ms, capturing 8 events.

Host-side activity: calling CUDA APIs took 217.68 µs (1.86% of the trace)
┌────┬──────────┬───────────┬─────────────────┬──────────────────────────┐
│ ID │    Start │      Time │            Name │ Details                  │
├────┼──────────┼───────────┼─────────────────┼──────────────────────────┤
│  2 │  7.39 µs │  14.07 µs │ cuMemAllocAsync │ 4.000 GiB, device memory │
│  6 │ 29.56 µs │ 202.42 µs │  cuLaunchKernel │ -                        │
└────┴──────────┴───────────┴─────────────────┴──────────────────────────┘

Device-side activity: GPU was busy for 11.48 ms (98.01% of the trace)
┌────┬──────────┬──────────┬─────────┬────────┬──────┬─────────────────────────────────
│ ID │    Start │     Time │ Threads │ Blocks │ Regs │ Name                           ⋯
├────┼──────────┼──────────┼─────────┼────────┼──────┼─────────────────────────────────
│  6 │ 229.6 µs │ 11.48 ms │     768 │    284 │   34 │ _Z16broadcast_kernel15CuKernel ⋯
└────┴──────────┴──────────┴─────────┴────────┴──────┴─────────────────────────────────</code></pre><p>Here, every call is prefixed with an ID, which can be used to correlate host and device events. For example, here we can see that the host-side <code>cuLaunchKernel</code> call with ID 6 corresponds to the device-side <code>broadcast</code> kernel.</p><h3 id="External-profilers"><a class="docs-heading-anchor" href="#External-profilers">External profilers</a><a id="External-profilers-1"></a><a class="docs-heading-anchor-permalink" href="#External-profilers" title="Permalink"></a></h3><p>If you want more details, or a graphical representation, we recommend using external profilers. To inform those external tools which code needs to be profiled (e.g., to exclude warm-up iterations or other noninteresting elements) you can also use <code>CUDA.@profile</code> to surround interesting code with:</p><pre><code class="language-julia hljs">julia&gt; a = CUDA.rand(1024,1024,1024);

julia&gt; sin.(a);  # warmup

julia&gt; CUDA.@profile sin.(a);
[ Info: This Julia session is already being profiled; defaulting to the external profiler.

julia&gt;</code></pre><p>Note that the external profiler is automatically detected, and makes <code>CUDA.@profile</code> switch to a mode where it merely activates an external profiler and does not do perform any profiling itself. In case the detection does not work, this mode can be forcibly activated by passing <code>external=true</code> to <code>CUDA.@profile</code>.</p><p>NVIDIA provides two tools for profiling CUDA applications: Nsight Systems and Nsight Compute for respectively timeline profiling and more detailed kernel analysis. Both tools are well-integrated with the Julia GPU packages, and make it possible to iteratively profile without having to restart Julia.</p><h4 id="NVIDIA-Nsight-Systems"><a class="docs-heading-anchor" href="#NVIDIA-Nsight-Systems">NVIDIA Nsight Systems</a><a id="NVIDIA-Nsight-Systems-1"></a><a class="docs-heading-anchor-permalink" href="#NVIDIA-Nsight-Systems" title="Permalink"></a></h4><p>Generally speaking, the first external profiler you should use is Nsight Systems, as it will give you a high-level overview of your application&#39;s performance characteristics. After downloading and installing the tool (a version might have been installed alongside with the CUDA toolkit, but it is recommended to download and install the latest version from the NVIDIA website), you need to launch Julia from the command-line, wrapped by the <code>nsys</code> utility from Nsight Systems:</p><pre><code class="nohighlight hljs">$ nsys launch julia</code></pre><p>You can then execute whatever code you want in the REPL, including e.g. loading Revise so that you can modify your application as you go. When you call into code that is wrapped by <code>CUDA.@profile</code>, the profiler will become active and generate a profile output file in the current folder:</p><pre><code class="language-julia hljs">julia&gt; using CUDA

julia&gt; a = CUDA.rand(1024,1024,1024);

julia&gt; sin.(a);

julia&gt; CUDA.@profile sin.(a);
start executed
Processing events...
Capturing symbol files...
Saving intermediate &quot;report.qdstrm&quot; file to disk...

Importing [===============================================================100%]
Saved report file to &quot;report.qdrep&quot;
stop executed</code></pre><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Even with a warm-up iteration, the first kernel or API call might seem to take significantly longer in the profiler. If you are analyzing short executions, instead of whole applications, repeat the operation twice (optionally separated by a call to <code>synchronize()</code> or wrapping in <code>CUDA.@sync</code>)</p></div></div><p>You can open the resulting <code>.qdrep</code> file with <code>nsight-sys</code>:</p><p><img src="../nsight_systems.png" alt="&quot;NVIDIA Nsight Systems&quot;"/></p><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>If Nsight Systems does not capture any kernel launch, even though you have used <code>CUDA.@profile</code>, try starting <code>nsys</code> with <code>--trace cuda</code>.</p></div></div><h4 id="NVIDIA-Nsight-Compute"><a class="docs-heading-anchor" href="#NVIDIA-Nsight-Compute">NVIDIA Nsight Compute</a><a id="NVIDIA-Nsight-Compute-1"></a><a class="docs-heading-anchor-permalink" href="#NVIDIA-Nsight-Compute" title="Permalink"></a></h4><p>If you want details on the execution properties of a single kernel, or inspect API interactions in detail, Nsight Compute is the tool for you. It is again possible to use this profiler with an interactive session of Julia, and debug or profile only those sections of your application that are marked with <code>CUDA.@profile</code>.</p><p>First, ensure that all (CUDA) packages that are involved in your application have been precompiled. Otherwise, you&#39;ll end up profiling the precompilation process, instead of the process where the actual work happens.</p><p>Then, launch Julia under the Nsight Compute CLI tool as follows:</p><pre><code class="nohighlight hljs">$ ncu --mode=launch julia</code></pre><p>You will get an interactive REPL, where you can execute whatever code you want:</p><pre><code class="language-julia hljs">julia&gt; using CUDA
# Julia hangs!</code></pre><p>As soon as you use CUDA.jl, your Julia process will hang. This is expected, as the tool breaks upon the very first call to the CUDA API, at which point you are expected to launch the Nsight Compute GUI utility, select <code>Interactive Profile</code> under <code>Activity</code>, and attach to the running session by selecting it in the list in the <code>Attach</code> pane:</p><p><img src="../nsight_compute-attach.png" alt="&quot;NVIDIA Nsight Compute - Attaching to a session&quot;"/></p><p>Note that this even works with remote systems, i.e., you can have Nsight Compute connect over ssh to a remote system where you run Julia under <code>ncu</code>.</p><p>Once you&#39;ve successfully attached to a Julia process, you will see that the tool has stopped execution on the call to <code>cuInit</code>. Now check <code>Profile &gt; Auto Profile</code> to make Nsight Compute gather statistics on our kernels, uncheck <code>Debug &gt; Break On API Error</code> to avoid halting the process when innocuous errors happen, and click <code>Debug &gt; Resume</code> to resume your application.</p><p>After doing so, our CLI session comes to life again, and we can execute the rest of our script:</p><pre><code class="language-julia hljs">julia&gt; a = CUDA.rand(1024,1024,1024);

julia&gt; sin.(a);

julia&gt; CUDA.@profile sin.(a);</code></pre><p>Once that&#39;s finished, the Nsight Compute GUI window will have plenty details on our kernel:</p><p><img src="../nsight_compute-kernel.png" alt="&quot;NVIDIA Nsight Compute - Kernel profiling&quot;"/></p><p>By default, this only collects a basic set of metrics. If you need more information on a specific kernel, select <code>detailed</code> or <code>full</code> in the <code>Metric Selection</code> pane and re-run your kernels. Note that collecting more metrics is also more expensive, sometimes even requiring multiple executions of your kernel. As such, it is recommended to only collect basic metrics by default, and only detailed or full metrics for kernels of interest.</p><p>At any point in time, you can also pause your application from the debug menu, and inspect the API calls that have been made:</p><p><img src="../nsight_compute-api.png" alt="&quot;NVIDIA Nsight Compute - API inspection&quot;"/></p><h4 id="Troubleshooting-Nsight-Compute"><a class="docs-heading-anchor" href="#Troubleshooting-Nsight-Compute">Troubleshooting Nsight Compute</a><a id="Troubleshooting-Nsight-Compute-1"></a><a class="docs-heading-anchor-permalink" href="#Troubleshooting-Nsight-Compute" title="Permalink"></a></h4><p>If you&#39;re running into issues, make sure you&#39;re using the same version of Nsight Compute on the host and the device, and make sure it&#39;s the latest version available. You do not need administrative permissions to install Nsight Compute, the <code>runfile</code> downloaded from the NVIDIA home page can be executed as a regular user.</p><h5 id="Kernel-sources-only-report-File-not-found"><a class="docs-heading-anchor" href="#Kernel-sources-only-report-File-not-found">Kernel sources only report <code>File not found</code></a><a id="Kernel-sources-only-report-File-not-found-1"></a><a class="docs-heading-anchor-permalink" href="#Kernel-sources-only-report-File-not-found" title="Permalink"></a></h5><p>When profiling a remote application, Nsight Compute will not be able to find the sources of kernels, and instead show <code>File not found</code> errors in the Source view. Although it is possible to point Nsight Compute to a local version of the remote file, it is recommended to enable &quot;Auto-Resolve Remote Source File&quot; in the global Profile preferences (Tools menu -&gt; Preferences). With that option set to &quot;Yes&quot;, clicking the &quot;Resolve&quot; button will automatically download and use the remote version of the requested source file.</p><h5 id="Could-not-load-library-&quot;libpcre2-8"><a class="docs-heading-anchor" href="#Could-not-load-library-&quot;libpcre2-8"><code>Could not load library &quot;libpcre2-8</code></a><a id="Could-not-load-library-&quot;libpcre2-8-1"></a><a class="docs-heading-anchor-permalink" href="#Could-not-load-library-&quot;libpcre2-8" title="Permalink"></a></h5><p>This is caused by an incompatibility between Julia and Nsight Compute, and should be fixed in the latest versions of Nsight Compute. If it&#39;s not possible to upgrade, the following workaround may help:</p><pre><code class="nohighlight hljs">LD_LIBRARY_PATH=$(/path/to/julia -e &#39;println(joinpath(Sys.BINDIR, Base.LIBDIR, &quot;julia&quot;))&#39;) ncu --mode=launch /path/to/julia</code></pre><h5 id="The-Julia-process-is-not-listed-in-the-&quot;Attach&quot;-tab"><a class="docs-heading-anchor" href="#The-Julia-process-is-not-listed-in-the-&quot;Attach&quot;-tab">The Julia process is not listed in the &quot;Attach&quot; tab</a><a id="The-Julia-process-is-not-listed-in-the-&quot;Attach&quot;-tab-1"></a><a class="docs-heading-anchor-permalink" href="#The-Julia-process-is-not-listed-in-the-&quot;Attach&quot;-tab" title="Permalink"></a></h5><p>Make sure that the port that is used by Nsight Compute (49152 by default) is accessible via ssh. To verify this, you can also try forwarding the port manually:</p><pre><code class="nohighlight hljs">ssh user@host.com -L 49152:localhost:49152</code></pre><p>Then, in the &quot;Connect to process&quot; window of Nsight Compute, add a connection to <code>localhost</code> instead of the remote host.</p><p>If SSH complains with <code>Address already in use</code>, that means the port is already in use. If you&#39;re using VSCode, try closing all instances as VSCode might automatically forward the port when launching Nsight Compute in a terminal within VSCode.</p><h5 id="Julia-in-Nsight-Compute-only-shows-the-Julia-logo,-not-the-REPL-prompt"><a class="docs-heading-anchor" href="#Julia-in-Nsight-Compute-only-shows-the-Julia-logo,-not-the-REPL-prompt">Julia in Nsight Compute only shows the Julia logo, not the REPL prompt</a><a id="Julia-in-Nsight-Compute-only-shows-the-Julia-logo,-not-the-REPL-prompt-1"></a><a class="docs-heading-anchor-permalink" href="#Julia-in-Nsight-Compute-only-shows-the-Julia-logo,-not-the-REPL-prompt" title="Permalink"></a></h5><p>In some versions of Nsight Compute, you might have to start Julia without the <code>--project</code> option and switch the environment from inside Julia.</p><h5 id="&quot;Disconnected-from-the-application&quot;-once-I-click-&quot;Resume&quot;"><a class="docs-heading-anchor" href="#&quot;Disconnected-from-the-application&quot;-once-I-click-&quot;Resume&quot;">&quot;Disconnected from the application&quot; once I click &quot;Resume&quot;</a><a id="&quot;Disconnected-from-the-application&quot;-once-I-click-&quot;Resume&quot;-1"></a><a class="docs-heading-anchor-permalink" href="#&quot;Disconnected-from-the-application&quot;-once-I-click-&quot;Resume&quot;" title="Permalink"></a></h5><p>Make sure that everything is precompiled before starting Julia with Nsight Compute, otherwise you end up profiling the precompilation process instead of your actual application.</p><p>Alternatively, disable auto profiling, resume, wait until the precompilation is finished, and then enable auto profiling again.</p><h5 id="I-only-see-the-&quot;API-Stream&quot;-tab-and-no-tab-with-details-on-my-kernel-on-the-right"><a class="docs-heading-anchor" href="#I-only-see-the-&quot;API-Stream&quot;-tab-and-no-tab-with-details-on-my-kernel-on-the-right">I only see the &quot;API Stream&quot; tab and no tab with details on my kernel on the right</a><a id="I-only-see-the-&quot;API-Stream&quot;-tab-and-no-tab-with-details-on-my-kernel-on-the-right-1"></a><a class="docs-heading-anchor-permalink" href="#I-only-see-the-&quot;API-Stream&quot;-tab-and-no-tab-with-details-on-my-kernel-on-the-right" title="Permalink"></a></h5><p>Scroll down in the &quot;API Stream&quot; tab and look for errors in the &quot;Details&quot; column. If it says &quot;The user does not have permission to access NVIDIA GPU Performance Counters on the target device&quot;, add this config:</p><pre><code class="nohighlight hljs"># cat /etc/modprobe.d/nvprof.conf
options nvidia NVreg_RestrictProfilingToAdminUsers=0</code></pre><p>The <code>nvidia.ko</code> kernel module needs to be reloaded after changing this configuration, and your system may require regenerating the initramfs or even a reboot. Refer to your distribution&#39;s documentation for details.</p><h5 id="Nsight-Compute-breaks-on-various-API-calls"><a class="docs-heading-anchor" href="#Nsight-Compute-breaks-on-various-API-calls">Nsight Compute breaks on various API calls</a><a id="Nsight-Compute-breaks-on-various-API-calls-1"></a><a class="docs-heading-anchor-permalink" href="#Nsight-Compute-breaks-on-various-API-calls" title="Permalink"></a></h5><p>Make sure <code>Break On API Error</code> is disabled in the <code>Debug</code> menu, as CUDA.jl purposefully triggers some API errors as part of its normal operation.</p><h2 id="Source-code-annotations"><a class="docs-heading-anchor" href="#Source-code-annotations">Source-code annotations</a><a id="Source-code-annotations-1"></a><a class="docs-heading-anchor-permalink" href="#Source-code-annotations" title="Permalink"></a></h2><p>If you want to put additional information in the profile, e.g. phases of your application, or expensive CPU operations, you can use the NVTX library via the NVTX.jl package:</p><pre><code class="language-julia hljs">using CUDA, NVTX

NVTX.@mark &quot;reached Y&quot;

NVTX.@range &quot;doing X&quot; begin
    ...
end

NVTX.@annotate function foo()
    ...
end</code></pre><p>For more details, refer to the documentation of the NVTX.jl package.</p><h2 id="Compiler-options"><a class="docs-heading-anchor" href="#Compiler-options">Compiler options</a><a id="Compiler-options-1"></a><a class="docs-heading-anchor-permalink" href="#Compiler-options" title="Permalink"></a></h2><p>Some tools, like Nsight Systems Compute, also make it possible to do source-level profiling. CUDA.jl will by default emit the necessary source line information, which you can disable by launching Julia with <code>-g0</code>. Conversely, launching with <code>-g2</code> will emit additional debug information, which can be useful in combination with tools like <code>cuda-gdb</code>, but might hurt performance or code size.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../usage/multigpu/">« Multiple GPUs</a><a class="docs-footer-nextpage" href="../kernel/">Kernel programming »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.7.0 on <span class="colophon-date" title="Friday 17 January 2025 13:14">Friday 17 January 2025</span>. Using Julia version 1.11.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
